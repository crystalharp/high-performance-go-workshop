High Performance Go
Tokyo 🌸
4 Dec 2016

Dave Cheney
dave@cheney.net
http://dave.cheney.net/
@davecheney

* License and Materials

This presentation is licensed under the [[https://creativecommons.org/licenses/by-sa/4.0/][Creative Commons Attribution-ShareAlike 4.0 International]] licence.

The materials for this presentation are available on GitHub:

.link https://github.com/davecheney/high-performance-go-workshop

You are encouraged to remix, transform, or build upon the material, providing you give appropriate credit and distribute your contributions under the same license.

If you have suggestions or corrections to this presentation, please raise [[https://github.com/davecheney/high-performance-go-workshop/issues][an issue on the GitHub project]].

* Agenda

This workshop is aimed at development teams who are building production Go applications intended for high scale deployment.

Today we are going to cover five areas:

- What does performance mean, what is possible?
- Benchmarking
- Performance measurement and profiling
- Memory management and GC tuning
- Concurrency

After each section we'll have time for questions.

* One more thing ...

This isn't a lecture, it's a conversation.

If you don't understand something, or think what you're hearing is incorrect, please ask.

* What does performance mean, what is possible?

* What does performance mean?

Before we talk about writing high performance code, we need to talk about the hardware that will execute this code.

What are its properties and how have they changed over time?

As software authors we have benefited from Moore's Law, the doubling of the number of available transistors on a chip every 18 months, for 50 years.

No other industry has experienced a _six_order_of_magnitude_ improvement in their tools in the space of a lifetime.

But this is all changing.

* The CPU

.image images/cpu.svg _ 600

每CPU晶体管数量不再增长

时钟速度这十年来也没有增长,每dollar的晶体管数量开始降低。

* More cores

.image images/Nehalem_Die_Shot_3.jpg _ 600

这十年以来,特别是服务器,CPU增长靠的是增加更多的核。

核的数量取决于散热和开销(heat dissipation and cost)

* Transistor size reductions have run out of steam

.image images/Mjc5MTM2Nw.png _ 580

缩减晶体管尺寸,或者在单个晶圆上放更多晶体管,已经行不通了。

CPU dies can be made larger, but that increases latency, and cost per transistor.

* Modern CPUs are optimised for bulk transfers

"Modern processors are a like nitro fueled funny cars, they excel at the quarter mile. Unfortunately modern programming languages are like Monte Carlo, they are full of twists and turns."
.caption David Ungar, OOPSLA (year unknown)

# https://youtu.be/4LG-RtcSYUQ?t=39m59s

Much of the improvement in performance in the last two decades has come from architectural improvements:

- out of order execution (super-scalar) 乱序执行
- speculative execution 预测执行
- vector (SSE) instructions 向量指令

Thus, modern CPUs are optimised for bulk transfers and bulk operations. At every level, the setup cost of an operation encourages you to work in bulk.现代CPU的设计是为了批量执行而优化的。

e.g. memory is not loaded per byte, but per multiple of cache lines, this is why alignment is becoming less of an issue. 

* Memory

单服务器内存呈几何级数增长。

.image images/latency.png _ 600

But, in terms of processor cycles lost, physical memory is still as far away as ever.

* Cache rules everything around it

.image images/xeon_e5_v4_hcc_rings.jpg _ 600

Cache支配了它周围的一切(rules everything around it)。但是它太小了,而且会一直这么小。光速决定了在确定的延迟下cache能有多大。

You can have a larger cache, but it will be slower because, in a universe where electricity travels a foot every nanosecond, distance equals latency.距离就是延迟。

* Network and disk I/O are still expensive

Network and disk I/O are still expensive, so expensive that the Go runtime will schedule something else while those operations are in progress.

.image images/media-20160803.jpg

* The free lunch is over

In 2005 Herb Sutter, the C++ committee leader, wrote an article entitled [[http://www.gotw.ca/publications/concurrency-ddj.htm][_The_free_lunch_is_over_]].

In his article Sutter discussed all the points I covered and asserted that programmers could no longer rely on faster hardware to fix slow programs—or slow programming languages.

Now, a decade later, there is no doubt that Herb Sutter was right. Memory is slow, caches are too small, CPU clock speeds are going backwards, and the simple world of a single threaded CPU is long gone.

* A fast programming language

It's time for the software to come to the party.

As [[https://www.youtube.com/watch?v=aiv1JOfMjm0][Rick Hudson noted at GopherCon]] in 2015, it's time for a programming language that works _with_ the limitations of today's hardware, rather than continue to ignore the reality that CPU designers find themselves.

So, for best performance on today's hardware in today's world, you need a programming language which:

- Is compiled, not interpreted.
- Permits efficient code to be written.
- Lets programmers talk about memory effectively, think structs vs java objects
- Has a compiler that produces efficient code, it has to be small code as well, because cache.

Obviously the language I'm talking about is the one we're here to discuss: Go.

* Discussion

Discussion:

- Do you agree that computers are no longer getting faster?

- Do you think we need to change languages to take advantage of modern hardware?

Further reading

.link https://www.youtube.com/watch?v=rKnDgT73v8s The Go Programming language (Nov 10, 2009)
.link https://www.youtube.com/watch?v=5kj5ApnhPAE OSCON 2010: Rob Pike, "Public Static Void"

* Benchmarking

* 基准测试

在给程序调优之前,你需要建立一个可信的基准来测量你的改动是使程序变好还是变坏了。

或者说,_“不要猜,要测量”_

This section focuses on how to construct useful benchmarks using the Go testing framework, and gives practical tips for avoiding the pitfalls.

Benchmarking is closely related to profiling, which we'll touch on during this section, then cover it in detail in the next.

* Benchmarking ground rules 基准测试的基本法则

基准测试前,必须要有个稳定的环境来获取可重复的结果。

- 主机必须空闲-不要在共享硬件上进行分析,不要在等待长时间基准测试运行的时候浏览网页
- 注意电池管理和发热控制
- 不要用虚拟机和共享云主机。对于持续测量有着太多的噪音。
- OS X版本在El Capitan之前有内核bug,升级或者避免在OS X上进行测量。这个bug同样影响其他*BSDs。

If you can afford it, buy dedicated performance test hardware. Rack it, disable all the power management and thermal scaling and never update the software on those machines.

对于其他人,准备前后两个例子,并多次运行来获取一致的结果。

* Using the testing package for benchmarking

The `testing` package has built in support for writing benchmarks.

.code examples/fib/fib_test.go /STARTFIB OMIT/,/ENDFIB OMIT/
.caption fib.go

.code examples/fib/fib_test.go /STARTBENCH OMIT/,/ENDBENCH OMIT/
.caption fib_test.go

DEMO: `go`test`-bench=.`./examples/fib`

* How benchmarks work

每个基准测试会运行`b.N`次,直到运行时间超过1秒

`b.N`从1开始,如果测试1秒内完成,`b.N`增加,测试再次运行。

`b.N`大概是这样的递增序列:1, 2, 3, 5, 10, 20, 30, 50, 100, ...

 % go test -bench=. ./examples/fib
 BenchmarkFib-4             30000             46408 ns/op
 PASS
 ok      _/Users/dfc/devel/high-performance-go-workshop/examples/fib     1.910s

_Beware:_ below the μs mark you will start to see the relativistic effects of instruction reordering and code alignment.

- Run benchmarks longer to get more accuracy; `go`test`-benchtime=10s`
- Run benchmarks multiple times; `go`test`-count=10`

_Tip:_ 如果需要,把这些规则写到`Makefile`里,这样所有人都可以进行对比了。

* Comparing benchmarks

对于可重复的结果,可以多次进行基准测试。
For repeatable results, you should run benchmarks multiple times.

You can do this manually, or use the `-count=` flag.

确定两组基准测试的性能差别很可能乏味并且容易出错。

Tools like [[https://godoc.org/rsc.io/benchstat][rsc.io/benchstat]] are useful for comparing results.

 % go test -c
 % mv fib.test fib.golden 

DEMO: Improve `Fib`

 % go test -c
 % ./fib.golden -test.bench=. -test.count=5 > old.txt
 % ./fib.test -test.bench=. -test.count=5 > new.txt
 % benchstat old.txt new.txt

DEMO: `benchstat`{old,new}.txt`

* Avoid benchmarking start up costs

Sometimes your benchmark has a once per run setup cost. `b.ResetTimer()` will can be used to ignore the time accrued in setup.

.code examples/reset.go /START1 OMIT/,/END1 OMIT/

If you have some expensive setup logic _per_loop_iteration, use `b.StopTimer()` and `b.StartTimer()` to pause the benchmark timer.

.code examples/reset.go /START2 OMIT/,/END2 OMIT/

* Benchmarking allocations

Allocation count and size is strongly correlated with benchmark time.

You can tell the `testing` framework to record the number of allocations made by code under test.
 
.code examples/benchmark.go

DEMO: `go`test`-run=^$`-bench=.`bufio`

_Note:_ you can also use the `go`test`-benchmem` flag to do the same for _all_ benchmarks.

DEMO: `go`test`-run=^$`-bench=.`-benchmem`bufio`

* Watch out for compiler optimisations

This example comes from [[https://github.com/golang/go/issues/14813#issue-140603392][issue 14813]]. How fast will this function benchmark?

.code examples/popcnt/popcnt_test.go /START OMIT/,/END OMIT/

* What happened?

 % go test -bench=. ./examples/popcnt

`popcnt` is a leaf function, so the compiler can inline it.

Because the function is inlined, the compiler can see it has no side effects, so the call is eliminated. This is what the compiler sees:

.code examples/popcnt/popcnt2_test.go /START OMIT/,/END OMIT/

同样的优化通过删除不必要的计算来使真实的代码更快。这个优化也删除了没有可观察的副作用的测试。 The same optimisations that make real code fast, by removing unnecessary computation, are the same ones that remove benchmarks that have no observable side effects.

This is only going to get more common as the Go compiler improves.

DEMO: show how to fix popcnt

* Benchmark mistakes

`for`循环对于基准测试是至关重要的。

Here are two incorrect benchmarks, can you explain what is wrong with them?

.code examples/benchfib/wrong_test.go /START OMIT/,/END OMIT/

* Discussion

Are there any questions?

Perhaps it is time for a break.

* 性能测量和分析

* Performance measurement and profiling

In the previous section we studied how to measure the performance of programs from the outside.

In this section we'll use profiling tools built into Go to investigate the operation of the program from the inside.

本节我们使用Go内置的分析工具来从内部调查程序的操作。

* pprof

The primary tool we're going to be talking about today is _pprof_.

[[https://github.com/google/pprof][pprof]] descends from the [[https://github.com/gperftools/gperftools][Google Perf Tools]] suite of tools.

`pprof` profiling is built into the Go runtime.

It consists of two parts:

- `runtime/pprof` package built into every Go program
- `go`tool`pprof` for investigating profiles.

pprof supports several types of profiling, we'll discuss three of these today: 

- CPU profiling.
- Memory profiling.
- Block (or blocking) profiling.

* CPU profiling

CPU分析是最常见的类型,也最明确。

CPU分析启用后,运行时环境会每10ms自我打断,并且记录当前运行的gorutines的堆栈信息。

一旦分析完成,我们就可以分析结果来确定最热的代码路径

结果中一个函数出现的次数越多,这个代码路径花费的时间占总运行时间的百分比越多。

* Memory profiling

内存分析在申请 _堆内存_ 的时候记录堆栈信息。

栈内存申请被认为是无代价的,在内存分析中 _不追踪_

和CPU分析一样,内存分析也是基于抽样的。默认每1000次申请抽样一次,频率可以修改。

因为内存分析基于抽样,并且追踪 _未使用_ 的 _内存申请_,用这个方法来确定总体内存使用量是很困难的。
Because of memory profiling is sample based and because it tracks _allocations_ not _use_, using memory profiling to determine your application's overall memory usage is difficult.

_个人意见:_ 不要使用内存分析来寻找内存泄露。有更好的方法来确定你的程序用了多少内存,后面会讨论。

* Block profiling

阻塞分析非常独特。

阻塞分析和CPU分析相似,但是它记录goroutine等待共享资源所花费的时间。

这样对确定程序的并发平静会有帮助。

Block分析能够在大量的goroutine本应工作却被阻塞的时候告诉你。
Block profiling can show you when a large number of goroutines _could_ make progress, but were _blocked_. Blocking includes:

- 在没有buffer的channel上发送或者接收
- 向已满的channel发送,或者从空的channel上接收
- 尝试对一个已经呗其他goroutine锁住的sync.Mutex加锁

阻塞分析是个非常特别的工具,在完全消除掉CPU和内存瓶颈之前不应该使用。

* One profile at at time

性能分析是有代价的。

性能分析对于程序性能有着温和但是可测量的影响 - 尤其在增加内存分析采样频率的时候。

大部分工具并不会阻止你开启多项分析。

如果同时启用多个性能分析,他们会注意到自己的相互作用,并且抛弃你的结果。
If you enable multiple profile's at the same time, they will observe their own interactions and throw off your results.

*不要同时启用一个以上的性能分析*

* Using pprof

Now that I've talked about what pprof can measure, I will talk about how to use pprof to analyse a profile.

pprof应该用两个参数运行

    go tool pprof /path/to/your/binary /path/to/your/profile

The `binary` argument *must* be the binary that produced this profile.

The `profile` argument *must* be the profile generated by this binary.

*Warning*: Because pprof also supports an online mode where it can fetch profiles from a running application over http, the pprof tool can be invoked without the name of your binary ([[https://github.com/golang/go/issues/10863][issue 10863]]):

    go tool pprof /tmp/c.pprof

*Do*not*do*this*or*pprof*will*report*your*profile*is*empty.*

* Using pprof (cont.)

This is a sample CPU profile:

	% go tool pprof $BINARY /tmp/c.p
	Entering interactive mode (type "help" for commands)
	(pprof) top
	Showing top 15 nodes out of 63 (cum >= 4.85s)
	      flat  flat%   sum%        cum   cum%
	    21.89s  9.84%  9.84%    128.32s 57.71%  net.(*netFD).Read
	    17.58s  7.91% 17.75%     40.28s 18.11%  runtime.exitsyscall
	    15.79s  7.10% 24.85%     15.79s  7.10%  runtime.newdefer
	    12.96s  5.83% 30.68%    151.41s 68.09%  test_frame/connection.(*ServerConn).readBytes
	    11.27s  5.07% 35.75%     23.35s 10.50%  runtime.reentersyscall
	    10.45s  4.70% 40.45%     82.77s 37.22%  syscall.Syscall
	     9.38s  4.22% 44.67%      9.38s  4.22%  runtime.deferproc_m
	     9.17s  4.12% 48.79%     12.73s  5.72%  exitsyscallfast
	     8.03s  3.61% 52.40%     11.86s  5.33%  runtime.casgstatus
	     7.66s  3.44% 55.85%      7.66s  3.44%  runtime.cas
	     7.59s  3.41% 59.26%      7.59s  3.41%  runtime.onM
	     6.42s  2.89% 62.15%    134.74s 60.60%  net.(*conn).Read
	     6.31s  2.84% 64.98%      6.31s  2.84%  runtime.writebarrierptr
	     6.26s  2.82% 67.80%     32.09s 14.43%  runtime.entersyscall

Often this output is hard to understand.

* Using pprof (cont.)

可视化的结果更容易理解

	% go tool pprof application /tmp/c.p
	Entering interactive mode (type "help" for commands)
	(pprof) web

Opens a web page with a graphical display of the profile.

.link images/profile.svg 

_Note_: visualisation requires graphviz.

I find this method to be superior to the text mode, I strongly recommend you try it.

pprof also supports these modes in a non interactive form with flags like `-svg`, `-pdf`, etc. See `go`tool`pprof`-help` for more details.

.link http://blog.golang.org/profiling-go-programs Further reading: Profiling Go programs
.link https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs Further reading: Debugging performance issues in Go programs

* Using pprof (cont.) 

The output of a memory profile can be similarly visualised.

    % go build -gcflags='-memprofile=/tmp/m.p'
    % go tool pprof --alloc_objects -svg $(go tool -n compile) /tmp/m.p > alloc_objects.svg
    % go tool pprof --inuse_objects -svg $(go tool -n compile) /tmp/m.p > inuse_objects.svg

Memory profiles come in two varieties

- Alloc objects reports the call site where each allocation was made

.link images/alloc_objects.svg

- Inuse objects reports the call site where an allocation was made _iff_ it was reachable at the end of the profile

.link images/inuse_objects.svg

DEMO: `examples/inuseallocs`

* Using pprof (cont.) 

Here is a visualisation of a block profile:

    % go test -run=XXX -bench=ClientServer -blockprofile=/tmp/b.p net/http
    % go tool pprof -svg http.test /tmp/b.p > block.svg

.link images/block.svg 

* Profiling benchmarks

`testing` 包内置了生成CPU,内存,阻塞分析的支持

- `-cpuprofile=$FILE` writes a CPU profile to `$FILE`. 
- `-memprofile=$FILE`, writes a memory profile to `$FILE`, `-memprofilerate=N` adjusts the profile rate to `1/N`.
- `-blockprofile=$FILE`, writes a block profile to `$FILE`.

Using any of these flags also preserves the binary.

    % go test -run=XXX -bench=. -cpuprofile=c.p bytes
    % go tool pprof bytes.test c.p

_Note:_ use `-run=XXX` to disable tests, you only want to profile benchmarks. You can also use `-run=^$` to accomplish the same thing.

* Profiling applications

Profiling `testing` benchmarks is useful for _microbenchmarks_.除了testing,我们还需要分析整个程序。

We use microbenchmarks inside the standard library to make sure individual packages do not regress, 但是如果我们想测试整个程序的性能呢?

The Go runtime's profiling interface is in the `runtime/pprof` package.

`runtime/pprof` 是个低级工具。因为历史原因,不同类型的分析使用的接口并不一致。

A few years ago I wrote a small package, [[https://github.com/pkg/profile][github.com/pkg/profile]], to make it easier to profile an application.

     import "github.com/pkg/profile"

     func main() {
           defer profile.Start().Stop()
           ...
     }

* Exercise

- Generate a profile from a piece of code you know well. If you don't have a code sample, try profiling `godoc`.

- If you were to generate a profile on one machine and inspect it on another, how would you do it?

* Framepointers

Go 1.7发布的同事带了一个amd64的编译器,它默认开启了frame pointers.

frame pointer是个寄存器,总是指向当前栈的最顶帧(the top of the current stack frame)。

Framepointers enable tools like `gdb(1)`, and `perf(1)` to understand the Go call stack.

We won't cover these tools in this workshop, but you can read and watch a presentation I gave on seven different ways to profile Go programs.

Further reading:

.link https://talks.godoc.org/github.com/davecheney/presentations/seven.slide Seven ways to profile a Go program (slides)
.link https://www.youtube.com/watch?v=2h_NFBFrciI Video (30 mins)
.link https://www.bigmarker.com/remote-meetup-go/Seven-ways-to-profile-a-Go-program Recording (60 mins)

* go tool trace

In Go 1.5, Dmitry Vyukov added a new kind of profiling to the runtime; [[https://golang.org/doc/go1.5#trace_command][execution trace profiling]].

可以看到一个程序的动态执行。

Captures with nanosecond precision:

- goroutine creation/start/end
- goroutine blocking/unblocking
- network blocking
- system calls
- GC events

Execution traces are essentially undocumented ☹️, see [[https://github.com/golang/go/issues/16526][github/go#16526]]

* go tool trace (cont.)

产生执行追踪(execution trace)

 % cd $(go env GOROOT)/test/bench/go1
 % go test -bench=HTTPClientServer -trace=/tmp/t.p

Viewing the trace:

 % go tool trace /tmp/t.p
 2016/08/13 17:01:04 Parsing trace...
 2016/08/13 17:01:04 Serializing trace...
 2016/08/13 17:01:05 Splitting trace...
 2016/08/13 17:01:06 Opening browser

Bonus: [[https://github.com/pkg/profile/releases/tag/v1.2.0][`github.com/pkg/profile`]] supports generating trace profiles.

 defer profile.Start(profile.TraceProfile).Stop()

* Exercises

- Create a trace profile of your application using `go`tool`trace` and [[https://github.com/pkg/profile/releases/tag/v1.2.0][`github.com/pkg/profile`]].

* Compiler optimisations 编译器优化

* Compiler optimisations

This section gives a brief background on three important optimisations that the Go compiler performs.

- Escape analysis 逃逸分析
- Inlining 内联
- Dead code elimination 无用代码删除

These are all handled in the front end of the compiler, while the code is still in its AST form; then the code is passed to the SSA compiler for further optimisation. 

* Escape analysis

一个合适的Go的实现能够储存每次堆内存分配的信息,但是这会给gc造成较大的压力。

然而栈是个适合存储局部变量的廉价存储,它也不需要垃圾收集。

以C和C++为代表的语言,堆、栈申请都是手动的,这是造成内存bug的一个普遍原因。

在Go中,如果一个变量的存活时间超过了函数调用的生命周期,编译器会自动把它移动到堆中。这被称为这个变量逃逸到了堆中。

同时,编译器也可以做相反的事情。它可以把new,make等预期申请在堆上的内存移动到栈里。

* Escape analysis (example)

Sum adds the ints between 1 and 100 and returns the result.

.code examples/esc/sum.go /START OMIT/,/END OMIT/
.caption examples/esc/sum.go

因为numbers切片只存在于Sum函数中,编译器会为这个切片在栈上分配存储100个整数的空间,而不是堆。这样就不需要对numbers进行垃圾收集,Sum函数返回的时候就会自动释放掉了。

* Investigating escape analysis

Prove it!

To print the compilers escape analysis decisions, use the `-m` flag.

 % go build -gcflags=-m examples/esc/sum.go
 # command-line-arguments
 examples/esc/sum.go:10: Sum make([]int, 100) does not escape
 examples/esc/sum.go:25: Sum() escapes to heap
 examples/esc/sum.go:25: main ... argument does not escape

Line 10 shows the compiler has correctly deduced that the result of `make([]int,`100)` does not escape to the heap.

We'll come back to line 25 soon.

* Escape analysis (example)

This example is a little contrived.

.code  examples/esc/center.go /START OMIT/,/END OMIT/

`NewPoint` creates a new `*Point` value, `p`. We pass `p` to the `Center` function which moves the point to a position in the center of the screen. Finally we print the values of `p.X` and `p.Y`.

* Escape analysis (example)

 % go build -gcflags=-m examples/esc/center.go 
 # command-line-arguments
 examples/esc/center.go:12: can inline Center
 examples/esc/center.go:19: inlining call to Center
 examples/esc/center.go:12: Center p does not escape
 examples/esc/center.go:20: p.X escapes to heap
 examples/esc/center.go:20: p.Y escapes to heap
 examples/esc/center.go:18: NewPoint new(Point) does not escape
 examples/esc/center.go:20: NewPoint ... argument does not escape

Even though `p` was allocated with the `new` function, it will not be stored on the heap, because no reference `p` escapes the `Center` function.

Question: What about line 20, if `p` doesn't escape, what is escaping to the heap?

 examples/esc/center.go:20: p.X escapes to heap
 examples/esc/center.go:20: p.Y escapes to heap

.link https://github.com/golang/go/issues/7714 Escape analysis is not perfect

* Exercise

- What is happening on line 25? Open up `examples/esc/sum.go` and see.
- Write a benchmark to provide that `Sum` does not allocate

* Inlining 

Go的函数调用有固定开销,栈和抢占检查。（可理解为函数调用开销）
In Go function calls in have a fixed overhead; stack and preemption check.

有些开销可以被硬件的分支预测改善,但是根据函数大小和时钟周期来算,还是有开销。 but it's still a cost in terms of function size and clock cycles.

Inlining是经典的避免这些开销的手段。

Inlining只影响叶子函数,就是那些不调用其他函数的函数。判断标准如下:

- 如果函数要做的功能比较多,那调用开销就可以忽略。所以函数超过某个大小就不会进行内联。That's why functions over a certain size (currently some count of instructions, plus a few operations which prevent inlining all together (eg. switch before Go 1.7)
- 相对来说,相比于真正的功能,小函数也有同样固定的调用开销。这些函数去做内联会有更多好处。

还有个原因是它让堆栈信息难以追踪。

* Inlining (example)

.play examples/max/max.go /START OMIT/,/END OMIT/

* Inlining (cont.)

Again we use the `-m` flag to view the compilers optimisation decision.

 % go build -gcflags=-m examples/max/max.go 
 # command-line-arguments
 examples/max/max.go:4: can inline Max
 examples/max/max.go:13: inlining call to Max

Compile `max.go` and see what the optimised version of `F()` became.

DEMO: `go`build`-gcflags="-m`-S"`examples/max/max.go`2>&1`|`less`

* Discussion

- Why did I declare `a` and `b` in `F()` to be constants?
- What happens if they are variables?
- What happens if they are passing into `F()` as parameters?

* Dead code elimination

Why is it important that `a` and `b` are constants?

After inlining, this is what the compiler saw

.play examples/max/max2.go /START OMIT/,/END OMIT/

- The call to `Max` has been inlined.
- If `a`>`b` then there is nothing to do, so the function returns. 
- If `a`<`b` then the branch is false and we fall through to `panic`
- But, because `a` and `b` are constants, we know that the branch will never be false, so the compiler can optimise `F()` to a return.

* Dead code elimination (cont.)

无用代码删除和内联一起通过删除那些不可达的循环和分支来减少代码数量。

You can take advantage of this to implement expensive debugging, and hide it behind

 const debug = false 

Combined with build tags this can be very useful.

Further reading:

.link http://dave.cheney.net/2014/09/28/using-build-to-switch-between-debug-and-release Using // +build to switch between debug and release builds
.link http://dave.cheney.net/2013/10/12/how-to-use-conditional-compilation-with-the-go-build-tool How to use conditional compilation with the go build tool

* Compiler flags Exercises

Compiler flags are provided with:

 go build -gcflags=$FLAGS

Investigate the operation of the following compiler functions:

- `-S` prints the (Go flavoured) assembly of the _package_ being compiled.
- `-l` controls the behaviour of the inliner; `-l` disables inlining, `-l`-l` increases it (more `-l` 's increases the compiler's appetite for inlining code). Experiment with the difference in compile time, program size, and run time.
- `-m` controls printing of optimisation decision like inlining, escape analysis. `-m`-m` prints more details about what the compiler was thinking.
- `-l`-N` disables all optimisations.

.link http://go-talks.appspot.com/github.com/rakyll/talks/gcinspect/talk.slide#1 Further reading: Codegen Inspection by Jaana Burcu Dogan

Perhaps we shall take a break now.

* Memory management and GC tuning 内存管理和GC调优

* Memory management and GC tuning

Go是一个垃圾收集的语言,这是不会改变的设计原则。

作为一个带垃圾收集的语言,Go程序的性能经常会被垃圾收集器的行为所影响。

除了算法选择,内存消耗是决定性能和可扩展性(performance and scalability)最重要的因素

本节讨论垃圾收集器的行为,如何测量程序的内存使用量,以及在垃圾收集器性能成为瓶颈的时候内存使用策略(strategies for lowering memory usage)

* Garbage collector world view

垃圾收集器的目标是给程序造成一种有无限内存可以使用的假象。

You may disagree with this statement, but this is the base assumption of how garbage collector designers think.

# A stop the world, mark sweep GC is the most efficient in terms of total run time; good for batch processing, simulation, etc.

Go GC的设计用途是低延迟服务器和交互式程序。

相对于高吞吐量,Go GC更偏向于低延迟。它把一些内存分配开销变成修改(mutator),以此控制之后的清除开销。

* Garbage collector design

The design of the Go GC has changed over the years

- Go 1.0, stop the world mark sweep collector based heavily on tcmalloc.
- Go 1.3, fully precise collector, wouldn't mistake big numbers on the heap for pointers, thus leaking memory.
- Go 1.5, new GC design, focusing on _latency_ over _throughput_.
- Go 1.6, GC improvements, handling larger heaps with lower latency.
- Go 1.7, small GC improvements, mainly refactoring.
- Go 1.8, further work to reduce STW times, now down to the 100 microsecond range.
- Go 1.9, ROC collector is an experiment to extend the idea of escape analysis per goroutine.

* Garbage collector monitoring

A simple way to obtain a general idea of how hard the garbage collector is working is to enable the output of GC logging.

These stats are always collected, but normally suppressed, you can enable their display by setting the `GODEBUG` environment variable.

 % env GODEBUG=gctrace=1 godoc -http=:8080
 gc 1 @0.017s 8%: 0.021+3.2+0.10+0.15+0.86 ms clock, 0.043+3.2+0+2.2/0.002/0.009+1.7 ms cpu, 5->6->1 MB, 4 MB goal, 4 P
 gc 2 @0.026s 12%: 0.11+4.9+0.12+1.6+0.54 ms clock, 0.23+4.9+0+3.0/0.50/0+1.0 ms cpu, 4->6->3 MB, 6 MB goal, 4 P
 gc 3 @0.035s 14%: 0.031+3.3+0.76+0.17+0.28 ms clock, 0.093+3.3+0+2.7/0.012/0+0.84 ms cpu, 4->5->3 MB, 3 MB goal, 4 P
 gc 4 @0.042s 17%: 0.067+5.1+0.15+0.29+0.95 ms clock, 0.20+5.1+0+3.0/0/0.070+2.8 ms cpu, 4->5->4 MB, 4 MB goal, 4 P
 gc 5 @0.051s 21%: 0.029+5.6+0.33+0.62+1.5 ms clock, 0.11+5.6+0+3.3/0.006/0.002+6.0 ms cpu, 5->6->4 MB, 5 MB goal, 4 P
 gc 6 @0.061s 23%: 0.080+7.6+0.17+0.22+0.45 ms clock, 0.32+7.6+0+5.4/0.001/0.11+1.8 ms cpu, 6->6->5 MB, 7 MB goal, 4 P
 gc 7 @0.071s 25%: 0.59+5.9+0.017+0.15+0.96 ms clock, 2.3+5.9+0+3.8/0.004/0.042+3.8 ms cpu, 6->8->6 MB, 8 MB goal, 4 P

The trace output gives a general measure of GC activity.

DEMO: Show `godoc` with `GODEBUG=gctrace=1` enabled

_Recommendation_: use this env var in production, it has no performance impact.

* Garbage collector monitoring (cont.)

Using `GODEBUG=gctrace=1` is good when you _know_ there is a problem, but for general telemetry on your Go application I recommend the `net/http/pprof` interface.

    import _ "net/http/pprof"

Importing the `net/http/pprof` package will register a handler at `/debug/pprof` with various runtime metrics, including:

- A list of all the running goroutines, `/debug/pprof/heap?debug=1`. 
- A report on the memory allocation statistics, `/debug/pprof/heap?debug=1`.

*Warning*: `net/http/pprof` will register itself with your default `http.ServeMux`.

Be careful as this will be visible if you use `http.ListenAndServe(address,`nil)`.

DEMO: `godoc`-http=:8080`, show `/debug/pprof`.

* Garbage collector tuning

Go运行时为GC调优提供了一个环境变量,GOGC

The formula for GOGC is as follows.

    goal = reachable * (1 + GOGC/100)

For example, if we currently have a 256MB heap, and `GOGC=100` (the default), when the heap fills up it will grow to

    512MB = 256MB * (1 + 100/100)

- `GOGC` 的值超过100的时候,堆增长快,GC压力变低。
- `GOGC` 的值低于100的时候,堆增长慢,GC压力变高。

The default value of 100 is _just_a_guide_. you should choose your own value _after_profiling_your_application_with_production_loads_.

* Reduce allocations

确定你的API接口能让调用者减少生成的垃圾。

Consider these two Read methods

    func (r *Reader) Read() ([]byte, error)
    func (r *Reader) Read(buf []byte) (int, error)

The first Read method takes no arguments and returns some data as a `[]byte`. The second takes a `[]byte` buffer and returns the amount of bytes read.

The first Read method will _always_ allocate a buffer, putting pressure on the GC. The second fills the buffer it was given.

_Exercise_: Can you name examples in the std lib which follow this pattern?

* strings and []bytes

Go中string是不可变的,[]byte是可变的。

大部分程序更喜欢用string,而IO都是用[]byte写的。

尽量避免[]byte到string的转换。这通常意味着为一个值选一种表达方式,string或者[]byte。如果数据是从网络或者磁盘读到的,通常是[]byte。

[[https://golang.org/pkg/bytes/][`bytes`]] 包里面有很多和[[https://golang.org/pkg/strings/][`strings`]]包相同的操作 — `Split`, `Compare`, `HasPrefix`, `Trim`, etc

Under the hood `strings` uses same assembly primitives as the `bytes` package.

* Using []byte as a map key

It is very common to use a `string` as a map key, but often you have a `[]byte`.

The compiler implements a specific optimisation for this case

     var m map[string]string
     v, ok := m[string(bytes)]

This will avoid the conversion of the byte slice to a string for the map lookup. This is very specific, it won't work if you do something like

     key := string(bytes)
     val, ok := m[key] 

* Avoid string concatenation

Go strings是不可变的。连接两个字符串会产生第三个,哪个会最快?

.code examples/concat/concat_test.go /START1 OMIT/,/END1 OMIT/
.code examples/concat/concat_test.go /START2 OMIT/,/END2 OMIT/
.code examples/concat/concat_test.go /START3 OMIT/,/END3 OMIT/
.code examples/concat/concat_test.go /START4 OMIT/,/END4 OMIT/

DEMO: `go`test`-bench=.`./examples/concat`

* Preallocate slices if the length is known

Append很方便,但是很浪费。

切片每次翻倍增加直到1024个元素,之后大概每次增加25%。b的容量在append一个元素后会是多少?

.play examples/grow.go /START OMIT/,/END OMIT/

If you use the append pattern you could be copying a lot of data and creating a lot of garbage.

* Preallocate slices if the length is known (cont.)

If know know the length of the slice beforehand, then pre-allocate the target to avoid copying and to make sure the target is exactly the right size. 

_Before:_

     var s []string
     for _, v := range fn() {
            s = append(s, v)
     }
     return s

_After:_

     vals := fn()
     s := make([]string, len(vals))
     for i, v := range vals {
            s[i] = v           
     }
     return s

* Using sync.Pool

The `sync` package comes with a `sync.Pool` type which is used to reuse common objects.

`sync.Pool` has no fixed size or maximum capacity. You add to it and take from it until a GC happens, then it is emptied unconditionally. 

.code examples/pool.go /START OMIT/,/END OMIT/

*Warning*: `sync.Pool` is not a cache. It can and will be emptied _at_any_time_.

Do not place important items in a `sync.Pool`, they will be discarded.

_Personal_opinion_: `sync.Pool` 很难安全使用,不要用它。

* Exercises

- Using `godoc` (or another program) observe the results of changing `GOGC` using `GODEBUG=gctrace=1`.

- Benchmark byte's string(byte) map keys

- Benchmark allocs from different concat strategies.

* Concurrency 并发

* Concurrency

Go的标志性特性就是它的轻量级并发模型。

虽然很廉价,但也不是没有代价。过度使用依然会导致不必要的性能问题。

这节会有一系列为了更好使用原生Go并发的建议和禁止。

* Goroutines

goroutines是go语言适应现代硬件的关键特性。

Goroutines非常易用,创建非常廉价,你可以认为它几乎没有代价。

Go的运行时环境能承担常态化数万的goroutines,但是不建议创建到数十万。

然而,每个goroutine最少需要使用2k内存作为它的栈内存。

2048 * 1,000,000 goroutines == 2GB,现在还什么都没干。

# Maybe this is a lot, maybe it isn't given the other usages of your application

* Know when to stop a goroutine

Goroutines的创建和运行都很廉价,但是因为内存问题依然有个有限的代价。它不能被无限的创建。

每个你使用go关键词来创建的goroutine,你必须知道怎样,何时能够让它退出。

如果你不知道,那这就会是个潜在的内存泄露。

在你的设计中,某些goroutines可能会一直运行到程序退出。这些goroutines要足够少,以避免成为这个规则下的异常。

*Never*start*a*goroutine*without*knowing*how*it*will*stop*.

# * Know when to stop a goroutine (cont.)
#
#TODO SHOW HOW TO STOP A GOROUTINE USING A DONE CHANNEL

* Go uses efficient network polling for some requests

Go运行时在处理网络IO的时候使用了非常高效的系统polling机制(kqueue, epoll, windows IOCP等)。很多等待中的goroutines可以对应同一个系统线程。

然而对于本地文件IO,Go并没有实现IO polling。每个对于*os.File的操作都会在执行中使用系统线程。

大量使用本地文件IO会导致程序产生出数十万的线程,可能比系统允许的还要多。 possibly more than your operating system allows.

你的磁盘子系统并不是给并发处理数十万IO请求而设计的。

* io.Reader and io.Writer are not buffered

`io.Reader` and `io.Writer` implementations are not buffered.
	
This includes `net.Conn` and `os.Stdout`.

Use `bufio.NewReader(r)` and `bufio.NewWriter(w)` to get a buffered reader and writer.

Don't forget to `Flush` or `Close` your buffered writers to flush the buffer to the underlying `Writer`.

* Watch out for IO multipliers in your application

如果你在写服务端进程,主要工作是多路传输(multiplex)通过网络连接的客户端,和程序中储存的数据。

大多数的服务器程序接收一个请求,做些处理,然后返回一个结果。听起来很简单,但是根据结果不同可以让客户端消耗掉大量的(甚至是无限的)服务器资源。下面是一些需要注意的点:

- 每个来的请求(incoming request)需要的IO请求(IO requests)的数量。单个客户端请求需要多少IO事件?可能平均1个,或者在多数请求被cache挡住时会少于1.
- 每个查询(query)需要多少读(amount of reads)。是常量,N+1,还是线性(读取整个表来生成最后一页结果)?

相对来说,如果内存慢,那IO就是非常慢,以至于你应该付出任何代价来避免它。最重要的是不要在请求的上下文中进行IO请求,不要让用户等待你的磁盘子系统写入磁盘,甚至读。

* Use streaming IO interfaces

只要有可能,避免把数据读入[]byte并且传来传去。

一个请求中,你可能最后会读取数兆的数据到内存中,这会给GC带来巨大的压力,也会增加程序的平均延时.

Instead use `io.Reader` and `io.Writer` to construct processing pipelines to cap the amount of memory in use per request.

For efficiency, consider implementing `io.ReaderFrom` / `io.WriterTo` if you use a lot of `io.Copy`. 这些接口效率更好,并且避免把内存拷贝到临时buffer里。

* Timeouts, timeouts, timeouts

永远不要在不知道一个IO操作花费多久的时候来使用它。

You need to set a timeout on every network request you make with `SetDeadline`, `SetReadDeadline`, `SetWriteDeadline`.

阻塞IO的数量需要被限制。用一个goroutine池,或者用有缓冲的channel作为信号量。

.code examples/semaphore.go /START OMIT/,/END OMIT/

* Defer is expensive, or is it?

`defer` is expensive because it has to record a closure for defer's arguments.

 defer mu.Unlock()

is equivalent to
 
 defer func() {
         mu.Unlock()
 }()

# talk about unwinding costs

如果要做的事很小,defer的代价就很昂贵。defer的经典例子是操作struct变量或者map查找时的解锁。在这些情况中你可以选择不用defer。

This is a case where readability and maintenance is sacrificed for a performance win. 

Always revisit these decisions.

.link https://github.com/golang/go/issues/9704#issuecomment-251003577

* Minimise cgo

cgo allows Go programs to call into C libraries. 

C code and Go code live in two different universes, cgo traverses the boundary between them.

This transition is not free and depending on where it exists in your code, the cost could be substantial.

cgo calls are similar to blocking IO, they consume a thread during operation.

Do not call out to C code in the middle of a tight loop.

* Actually, avoid cgo

cgo has a high overhead.

For best performance I recommend avoiding cgo in your applications.

- If the C code takes a long time, cgo overhead is not as important.
- If you're using cgo to call a very short C function, where the overhead is the most noticeable, rewrite that code in Go -- by definition it's short.
- If you're using a large piece of expensive C code is called in a tight loop, why are you using Go?

Is there anyone who's using cgo to call expensive C code frequently?

.link http://dave.cheney.net/2016/01/18/cgo-is-not-go Further reading: cgo is not Go.

* Always use the latest released version of Go

Old versions of Go will never get better. They will never get bug fixes or optimisations.

- Go 1.4 should not be used.
- Go 1.5 and 1.6 had a slower compiler, but it produces faster code, and has a faster GC.
- Go 1.7 delivered roughly a 30% improvement in compilation speed over 1.6, a 2x improvement in linking speed (better than any previous version of Go).
- Go 1.8 will deliver a smaller improvement in compilation speed (at this point), but a significant improvement in code quality for non Intel architectures.

Old version of Go receive no updates. Do not use them. Use the latest and you will get the best performance.

.link http://dave.cheney.net/2016/04/02/go-1-7-toolchain-improvements Go 1.7 toolchain improvements
.link http://dave.cheney.net/2016/09/18/go-1-8-performance-improvements-one-month-in Go 1.8 performance improvements

* Discussion

Any questions?

* Conclusion 总结

* Conclusion

Always write the simplest code you can, the compiler is optimised for _normal_ code.

Start with the simplest possible code.

_Measure_.

如果性能足够好,那就不要再优化了。并不是所有东西都需要优化,只有代码中最热的部分。

随着程序的增长,或者流量模式的进化,性能热点会变化。

如果不是影响关键性能,不要留下复杂代码。如果瓶颈不再这里,就用简单操作去重写。

* Conclusion (cont.)

给代码做性能分析来确定瓶颈,不要靠猜。

短代码就是快代码。Go不是C++, do not expect the compiler to unravel complicated abstractions.

Shorter code is _smaller_ code; which is important for the CPU's cache.

Pay very close attention to allocations, avoid unnecessary allocation where possible.

* Don't trade performance for reliability

"I can make things very fast if they don't have to be correct."
.caption Russ Cox

"Readable means reliable"
.caption Rob Pike

性能和可靠性同等重要。

I see little value in making a very fast server that panics, deadlocks or OOMs on a regular basis.

不要牺牲可靠性来换取性能。
