High Performance Go
Tokyo ğŸŒ¸
4 Dec 2016

Dave Cheney
dave@cheney.net
http://dave.cheney.net/
@davecheney

* License and Materials

This presentation is licensed under the [[https://creativecommons.org/licenses/by-sa/4.0/][Creative Commons Attribution-ShareAlike 4.0 International]] licence.

The materials for this presentation are available on GitHub:

.link https://github.com/davecheney/high-performance-go-workshop

You are encouraged to remix, transform, or build upon the material, providing you give appropriate credit and distribute your contributions under the same license.

If you have suggestions or corrections to this presentation, please raise [[https://github.com/davecheney/high-performance-go-workshop/issues][an issue on the GitHub project]].

* Agenda

This workshop is aimed at development teams who are building production Go applications intended for high scale deployment.

Today we are going to cover five areas:

- What does performance mean, what is possible?
- Benchmarking
- Performance measurement and profiling
- Memory management and GC tuning
- Concurrency

After each section we'll have time for questions.

* One more thing ...

This isn't a lecture, it's a conversation.

If you don't understand something, or think what you're hearing is incorrect, please ask.

* What does performance mean, what is possible?

* What does performance mean?

Before we talk about writing high performance code, we need to talk about the hardware that will execute this code.

What are its properties and how have they changed over time?

As software authors we have benefited from Moore's Law, the doubling of the number of available transistors on a chip every 18 months, for 50 years.

No other industry has experienced a _six_order_of_magnitude_ improvement in their tools in the space of a lifetime.

But this is all changing.

* The CPU

.image images/cpu.svg _ 600

æ¯CPUæ™¶ä½“ç®¡æ•°é‡ä¸å†å¢é•¿

æ—¶é’Ÿé€Ÿåº¦è¿™åå¹´æ¥ä¹Ÿæ²¡æœ‰å¢é•¿,æ¯dollarçš„æ™¶ä½“ç®¡æ•°é‡å¼€å§‹é™ä½ã€‚

* More cores

.image images/Nehalem_Die_Shot_3.jpg _ 600

è¿™åå¹´ä»¥æ¥,ç‰¹åˆ«æ˜¯æœåŠ¡å™¨,CPUå¢é•¿é çš„æ˜¯å¢åŠ æ›´å¤šçš„æ ¸ã€‚

æ ¸çš„æ•°é‡å–å†³äºæ•£çƒ­å’Œå¼€é”€(heat dissipation and cost)

* Transistor size reductions have run out of steam

.image images/Mjc5MTM2Nw.png _ 580

ç¼©å‡æ™¶ä½“ç®¡å°ºå¯¸,æˆ–è€…åœ¨å•ä¸ªæ™¶åœ†ä¸Šæ”¾æ›´å¤šæ™¶ä½“ç®¡,å·²ç»è¡Œä¸é€šäº†ã€‚

CPU dies can be made larger, but that increases latency, and cost per transistor.

* Modern CPUs are optimised for bulk transfers

"Modern processors are a like nitro fueled funny cars, they excel at the quarter mile. Unfortunately modern programming languages are like Monte Carlo, they are full of twists and turns."
.caption David Ungar, OOPSLA (year unknown)

# https://youtu.be/4LG-RtcSYUQ?t=39m59s

Much of the improvement in performance in the last two decades has come from architectural improvements:

- out of order execution (super-scalar) ä¹±åºæ‰§è¡Œ
- speculative execution é¢„æµ‹æ‰§è¡Œ
- vector (SSE) instructions å‘é‡æŒ‡ä»¤

Thus, modern CPUs are optimised for bulk transfers and bulk operations. At every level, the setup cost of an operation encourages you to work in bulk.ç°ä»£CPUçš„è®¾è®¡æ˜¯ä¸ºäº†æ‰¹é‡æ‰§è¡Œè€Œä¼˜åŒ–çš„ã€‚

e.g. memory is not loaded per byte, but per multiple of cache lines, this is why alignment is becoming less of an issue. 

* Memory

å•æœåŠ¡å™¨å†…å­˜å‘ˆå‡ ä½•çº§æ•°å¢é•¿ã€‚

.image images/latency.png _ 600

But, in terms of processor cycles lost, physical memory is still as far away as ever.

* Cache rules everything around it

.image images/xeon_e5_v4_hcc_rings.jpg _ 600

Cacheæ”¯é…äº†å®ƒå‘¨å›´çš„ä¸€åˆ‡(rules everything around it)ã€‚ä½†æ˜¯å®ƒå¤ªå°äº†,è€Œä¸”ä¼šä¸€ç›´è¿™ä¹ˆå°ã€‚å…‰é€Ÿå†³å®šäº†åœ¨ç¡®å®šçš„å»¶è¿Ÿä¸‹cacheèƒ½æœ‰å¤šå¤§ã€‚

You can have a larger cache, but it will be slower because, in a universe where electricity travels a foot every nanosecond, distance equals latency.è·ç¦»å°±æ˜¯å»¶è¿Ÿã€‚

* Network and disk I/O are still expensive

Network and disk I/O are still expensive, so expensive that the Go runtime will schedule something else while those operations are in progress.

.image images/media-20160803.jpg

* The free lunch is over

In 2005 Herb Sutter, the C++ committee leader, wrote an article entitled [[http://www.gotw.ca/publications/concurrency-ddj.htm][_The_free_lunch_is_over_]].

In his article Sutter discussed all the points I covered and asserted that programmers could no longer rely on faster hardware to fix slow programsâ€”or slow programming languages.

Now, a decade later, there is no doubt that Herb Sutter was right. Memory is slow, caches are too small, CPU clock speeds are going backwards, and the simple world of a single threaded CPU is long gone.

* A fast programming language

It's time for the software to come to the party.

As [[https://www.youtube.com/watch?v=aiv1JOfMjm0][Rick Hudson noted at GopherCon]] in 2015, it's time for a programming language that works _with_ the limitations of today's hardware, rather than continue to ignore the reality that CPU designers find themselves.

So, for best performance on today's hardware in today's world, you need a programming language which:

- Is compiled, not interpreted.
- Permits efficient code to be written.
- Lets programmers talk about memory effectively, think structs vs java objects
- Has a compiler that produces efficient code, it has to be small code as well, because cache.

Obviously the language I'm talking about is the one we're here to discuss: Go.

* Discussion

Discussion:

- Do you agree that computers are no longer getting faster?

- Do you think we need to change languages to take advantage of modern hardware?

Further reading

.link https://www.youtube.com/watch?v=rKnDgT73v8s The Go Programming language (Nov 10, 2009)
.link https://www.youtube.com/watch?v=5kj5ApnhPAE OSCON 2010: Rob Pike, "Public Static Void"

* Benchmarking

* åŸºå‡†æµ‹è¯•

åœ¨ç»™ç¨‹åºè°ƒä¼˜ä¹‹å‰,ä½ éœ€è¦å»ºç«‹ä¸€ä¸ªå¯ä¿¡çš„åŸºå‡†æ¥æµ‹é‡ä½ çš„æ”¹åŠ¨æ˜¯ä½¿ç¨‹åºå˜å¥½è¿˜æ˜¯å˜åäº†ã€‚

æˆ–è€…è¯´,_â€œä¸è¦çŒœ,è¦æµ‹é‡â€_

This section focuses on how to construct useful benchmarks using the Go testing framework, and gives practical tips for avoiding the pitfalls.

Benchmarking is closely related to profiling, which we'll touch on during this section, then cover it in detail in the next.

* Benchmarking ground rules åŸºå‡†æµ‹è¯•çš„åŸºæœ¬æ³•åˆ™

åŸºå‡†æµ‹è¯•å‰,å¿…é¡»è¦æœ‰ä¸ªç¨³å®šçš„ç¯å¢ƒæ¥è·å–å¯é‡å¤çš„ç»“æœã€‚

- ä¸»æœºå¿…é¡»ç©ºé—²-ä¸è¦åœ¨å…±äº«ç¡¬ä»¶ä¸Šè¿›è¡Œåˆ†æ,ä¸è¦åœ¨ç­‰å¾…é•¿æ—¶é—´åŸºå‡†æµ‹è¯•è¿è¡Œçš„æ—¶å€™æµè§ˆç½‘é¡µ
- æ³¨æ„ç”µæ± ç®¡ç†å’Œå‘çƒ­æ§åˆ¶
- ä¸è¦ç”¨è™šæ‹Ÿæœºå’Œå…±äº«äº‘ä¸»æœºã€‚å¯¹äºæŒç»­æµ‹é‡æœ‰ç€å¤ªå¤šçš„å™ªéŸ³ã€‚
- OS Xç‰ˆæœ¬åœ¨El Capitanä¹‹å‰æœ‰å†…æ ¸bug,å‡çº§æˆ–è€…é¿å…åœ¨OS Xä¸Šè¿›è¡Œæµ‹é‡ã€‚è¿™ä¸ªbugåŒæ ·å½±å“å…¶ä»–*BSDsã€‚

If you can afford it, buy dedicated performance test hardware. Rack it, disable all the power management and thermal scaling and never update the software on those machines.

å¯¹äºå…¶ä»–äºº,å‡†å¤‡å‰åä¸¤ä¸ªä¾‹å­,å¹¶å¤šæ¬¡è¿è¡Œæ¥è·å–ä¸€è‡´çš„ç»“æœã€‚

* Using the testing package for benchmarking

The `testing` package has built in support for writing benchmarks.

.code examples/fib/fib_test.go /STARTFIB OMIT/,/ENDFIB OMIT/
.caption fib.go

.code examples/fib/fib_test.go /STARTBENCH OMIT/,/ENDBENCH OMIT/
.caption fib_test.go

DEMO: `go`test`-bench=.`./examples/fib`

* How benchmarks work

æ¯ä¸ªåŸºå‡†æµ‹è¯•ä¼šè¿è¡Œ`b.N`æ¬¡,ç›´åˆ°è¿è¡Œæ—¶é—´è¶…è¿‡1ç§’

`b.N`ä»1å¼€å§‹,å¦‚æœæµ‹è¯•1ç§’å†…å®Œæˆ,`b.N`å¢åŠ ,æµ‹è¯•å†æ¬¡è¿è¡Œã€‚

`b.N`å¤§æ¦‚æ˜¯è¿™æ ·çš„é€’å¢åºåˆ—:1, 2, 3, 5, 10, 20, 30, 50, 100, ...

 % go test -bench=. ./examples/fib
 BenchmarkFib-4             30000             46408 ns/op
 PASS
 ok      _/Users/dfc/devel/high-performance-go-workshop/examples/fib     1.910s

_Beware:_ below the Î¼s mark you will start to see the relativistic effects of instruction reordering and code alignment.

- Run benchmarks longer to get more accuracy; `go`test`-benchtime=10s`
- Run benchmarks multiple times; `go`test`-count=10`

_Tip:_ å¦‚æœéœ€è¦,æŠŠè¿™äº›è§„åˆ™å†™åˆ°`Makefile`é‡Œ,è¿™æ ·æ‰€æœ‰äººéƒ½å¯ä»¥è¿›è¡Œå¯¹æ¯”äº†ã€‚

* Comparing benchmarks

å¯¹äºå¯é‡å¤çš„ç»“æœ,å¯ä»¥å¤šæ¬¡è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚
For repeatable results, you should run benchmarks multiple times.

You can do this manually, or use the `-count=` flag.

ç¡®å®šä¸¤ç»„åŸºå‡†æµ‹è¯•çš„æ€§èƒ½å·®åˆ«å¾ˆå¯èƒ½ä¹å‘³å¹¶ä¸”å®¹æ˜“å‡ºé”™ã€‚

Tools like [[https://godoc.org/rsc.io/benchstat][rsc.io/benchstat]] are useful for comparing results.

 % go test -c
 % mv fib.test fib.golden 

DEMO: Improve `Fib`

 % go test -c
 % ./fib.golden -test.bench=. -test.count=5 > old.txt
 % ./fib.test -test.bench=. -test.count=5 > new.txt
 % benchstat old.txt new.txt

DEMO: `benchstat`{old,new}.txt`

* Avoid benchmarking start up costs

Sometimes your benchmark has a once per run setup cost. `b.ResetTimer()` will can be used to ignore the time accrued in setup.

.code examples/reset.go /START1 OMIT/,/END1 OMIT/

If you have some expensive setup logic _per_loop_iteration, use `b.StopTimer()` and `b.StartTimer()` to pause the benchmark timer.

.code examples/reset.go /START2 OMIT/,/END2 OMIT/

* Benchmarking allocations

Allocation count and size is strongly correlated with benchmark time.

You can tell the `testing` framework to record the number of allocations made by code under test.
 
.code examples/benchmark.go

DEMO: `go`test`-run=^$`-bench=.`bufio`

_Note:_ you can also use the `go`test`-benchmem` flag to do the same for _all_ benchmarks.

DEMO: `go`test`-run=^$`-bench=.`-benchmem`bufio`

* Watch out for compiler optimisations

This example comes from [[https://github.com/golang/go/issues/14813#issue-140603392][issue 14813]]. How fast will this function benchmark?

.code examples/popcnt/popcnt_test.go /START OMIT/,/END OMIT/

* What happened?

 % go test -bench=. ./examples/popcnt

`popcnt` is a leaf function, so the compiler can inline it.

Because the function is inlined, the compiler can see it has no side effects, so the call is eliminated. This is what the compiler sees:

.code examples/popcnt/popcnt2_test.go /START OMIT/,/END OMIT/

åŒæ ·çš„ä¼˜åŒ–é€šè¿‡åˆ é™¤ä¸å¿…è¦çš„è®¡ç®—æ¥ä½¿çœŸå®çš„ä»£ç æ›´å¿«ã€‚è¿™ä¸ªä¼˜åŒ–ä¹Ÿåˆ é™¤äº†æ²¡æœ‰å¯è§‚å¯Ÿçš„å‰¯ä½œç”¨çš„æµ‹è¯•ã€‚ The same optimisations that make real code fast, by removing unnecessary computation, are the same ones that remove benchmarks that have no observable side effects.

This is only going to get more common as the Go compiler improves.

DEMO: show how to fix popcnt

* Benchmark mistakes

`for`å¾ªç¯å¯¹äºåŸºå‡†æµ‹è¯•æ˜¯è‡³å…³é‡è¦çš„ã€‚

Here are two incorrect benchmarks, can you explain what is wrong with them?

.code examples/benchfib/wrong_test.go /START OMIT/,/END OMIT/

* Discussion

Are there any questions?

Perhaps it is time for a break.

* æ€§èƒ½æµ‹é‡å’Œåˆ†æ

* Performance measurement and profiling

In the previous section we studied how to measure the performance of programs from the outside.

In this section we'll use profiling tools built into Go to investigate the operation of the program from the inside.

æœ¬èŠ‚æˆ‘ä»¬ä½¿ç”¨Goå†…ç½®çš„åˆ†æå·¥å…·æ¥ä»å†…éƒ¨è°ƒæŸ¥ç¨‹åºçš„æ“ä½œã€‚

* pprof

The primary tool we're going to be talking about today is _pprof_.

[[https://github.com/google/pprof][pprof]] descends from the [[https://github.com/gperftools/gperftools][Google Perf Tools]] suite of tools.

`pprof` profiling is built into the Go runtime.

It consists of two parts:

- `runtime/pprof` package built into every Go program
- `go`tool`pprof` for investigating profiles.

pprof supports several types of profiling, we'll discuss three of these today: 

- CPU profiling.
- Memory profiling.
- Block (or blocking) profiling.

* CPU profiling

CPUåˆ†ææ˜¯æœ€å¸¸è§çš„ç±»å‹,ä¹Ÿæœ€æ˜ç¡®ã€‚

CPUåˆ†æå¯ç”¨å,è¿è¡Œæ—¶ç¯å¢ƒä¼šæ¯10msè‡ªæˆ‘æ‰“æ–­,å¹¶ä¸”è®°å½•å½“å‰è¿è¡Œçš„gorutinesçš„å †æ ˆä¿¡æ¯ã€‚

ä¸€æ—¦åˆ†æå®Œæˆ,æˆ‘ä»¬å°±å¯ä»¥åˆ†æç»“æœæ¥ç¡®å®šæœ€çƒ­çš„ä»£ç è·¯å¾„

ç»“æœä¸­ä¸€ä¸ªå‡½æ•°å‡ºç°çš„æ¬¡æ•°è¶Šå¤š,è¿™ä¸ªä»£ç è·¯å¾„èŠ±è´¹çš„æ—¶é—´å æ€»è¿è¡Œæ—¶é—´çš„ç™¾åˆ†æ¯”è¶Šå¤šã€‚

* Memory profiling

å†…å­˜åˆ†æåœ¨ç”³è¯· _å †å†…å­˜_ çš„æ—¶å€™è®°å½•å †æ ˆä¿¡æ¯ã€‚

æ ˆå†…å­˜ç”³è¯·è¢«è®¤ä¸ºæ˜¯æ— ä»£ä»·çš„,åœ¨å†…å­˜åˆ†æä¸­ _ä¸è¿½è¸ª_

å’ŒCPUåˆ†æä¸€æ ·,å†…å­˜åˆ†æä¹Ÿæ˜¯åŸºäºæŠ½æ ·çš„ã€‚é»˜è®¤æ¯1000æ¬¡ç”³è¯·æŠ½æ ·ä¸€æ¬¡,é¢‘ç‡å¯ä»¥ä¿®æ”¹ã€‚

å› ä¸ºå†…å­˜åˆ†æåŸºäºæŠ½æ ·,å¹¶ä¸”è¿½è¸ª _æœªä½¿ç”¨_ çš„ _å†…å­˜ç”³è¯·_,ç”¨è¿™ä¸ªæ–¹æ³•æ¥ç¡®å®šæ€»ä½“å†…å­˜ä½¿ç”¨é‡æ˜¯å¾ˆå›°éš¾çš„ã€‚
Because of memory profiling is sample based and because it tracks _allocations_ not _use_, using memory profiling to determine your application's overall memory usage is difficult.

_ä¸ªäººæ„è§:_ ä¸è¦ä½¿ç”¨å†…å­˜åˆ†ææ¥å¯»æ‰¾å†…å­˜æ³„éœ²ã€‚æœ‰æ›´å¥½çš„æ–¹æ³•æ¥ç¡®å®šä½ çš„ç¨‹åºç”¨äº†å¤šå°‘å†…å­˜,åé¢ä¼šè®¨è®ºã€‚

* Block profiling

é˜»å¡åˆ†æéå¸¸ç‹¬ç‰¹ã€‚

é˜»å¡åˆ†æå’ŒCPUåˆ†æç›¸ä¼¼,ä½†æ˜¯å®ƒè®°å½•goroutineç­‰å¾…å…±äº«èµ„æºæ‰€èŠ±è´¹çš„æ—¶é—´ã€‚

è¿™æ ·å¯¹ç¡®å®šç¨‹åºçš„å¹¶å‘å¹³é™ä¼šæœ‰å¸®åŠ©ã€‚

Blockåˆ†æèƒ½å¤Ÿåœ¨å¤§é‡çš„goroutineæœ¬åº”å·¥ä½œå´è¢«é˜»å¡çš„æ—¶å€™å‘Šè¯‰ä½ ã€‚
Block profiling can show you when a large number of goroutines _could_ make progress, but were _blocked_. Blocking includes:

- åœ¨æ²¡æœ‰bufferçš„channelä¸Šå‘é€æˆ–è€…æ¥æ”¶
- å‘å·²æ»¡çš„channelå‘é€,æˆ–è€…ä»ç©ºçš„channelä¸Šæ¥æ”¶
- å°è¯•å¯¹ä¸€ä¸ªå·²ç»å‘—å…¶ä»–goroutineé”ä½çš„sync.MutexåŠ é”

é˜»å¡åˆ†ææ˜¯ä¸ªéå¸¸ç‰¹åˆ«çš„å·¥å…·,åœ¨å®Œå…¨æ¶ˆé™¤æ‰CPUå’Œå†…å­˜ç“¶é¢ˆä¹‹å‰ä¸åº”è¯¥ä½¿ç”¨ã€‚

* One profile at at time

æ€§èƒ½åˆ†ææ˜¯æœ‰ä»£ä»·çš„ã€‚

æ€§èƒ½åˆ†æå¯¹äºç¨‹åºæ€§èƒ½æœ‰ç€æ¸©å’Œä½†æ˜¯å¯æµ‹é‡çš„å½±å“ - å°¤å…¶åœ¨å¢åŠ å†…å­˜åˆ†æé‡‡æ ·é¢‘ç‡çš„æ—¶å€™ã€‚

å¤§éƒ¨åˆ†å·¥å…·å¹¶ä¸ä¼šé˜»æ­¢ä½ å¼€å¯å¤šé¡¹åˆ†æã€‚

å¦‚æœåŒæ—¶å¯ç”¨å¤šä¸ªæ€§èƒ½åˆ†æ,ä»–ä»¬ä¼šæ³¨æ„åˆ°è‡ªå·±çš„ç›¸äº’ä½œç”¨,å¹¶ä¸”æŠ›å¼ƒä½ çš„ç»“æœã€‚
If you enable multiple profile's at the same time, they will observe their own interactions and throw off your results.

*ä¸è¦åŒæ—¶å¯ç”¨ä¸€ä¸ªä»¥ä¸Šçš„æ€§èƒ½åˆ†æ*

* Using pprof

Now that I've talked about what pprof can measure, I will talk about how to use pprof to analyse a profile.

pprofåº”è¯¥ç”¨ä¸¤ä¸ªå‚æ•°è¿è¡Œ

    go tool pprof /path/to/your/binary /path/to/your/profile

The `binary` argument *must* be the binary that produced this profile.

The `profile` argument *must* be the profile generated by this binary.

*Warning*: Because pprof also supports an online mode where it can fetch profiles from a running application over http, the pprof tool can be invoked without the name of your binary ([[https://github.com/golang/go/issues/10863][issue 10863]]):

    go tool pprof /tmp/c.pprof

*Do*not*do*this*or*pprof*will*report*your*profile*is*empty.*

* Using pprof (cont.)

This is a sample CPU profile:

	% go tool pprof $BINARY /tmp/c.p
	Entering interactive mode (type "help" for commands)
	(pprof) top
	Showing top 15 nodes out of 63 (cum >= 4.85s)
	      flat  flat%   sum%        cum   cum%
	    21.89s  9.84%  9.84%    128.32s 57.71%  net.(*netFD).Read
	    17.58s  7.91% 17.75%     40.28s 18.11%  runtime.exitsyscall
	    15.79s  7.10% 24.85%     15.79s  7.10%  runtime.newdefer
	    12.96s  5.83% 30.68%    151.41s 68.09%  test_frame/connection.(*ServerConn).readBytes
	    11.27s  5.07% 35.75%     23.35s 10.50%  runtime.reentersyscall
	    10.45s  4.70% 40.45%     82.77s 37.22%  syscall.Syscall
	     9.38s  4.22% 44.67%      9.38s  4.22%  runtime.deferproc_m
	     9.17s  4.12% 48.79%     12.73s  5.72%  exitsyscallfast
	     8.03s  3.61% 52.40%     11.86s  5.33%  runtime.casgstatus
	     7.66s  3.44% 55.85%      7.66s  3.44%  runtime.cas
	     7.59s  3.41% 59.26%      7.59s  3.41%  runtime.onM
	     6.42s  2.89% 62.15%    134.74s 60.60%  net.(*conn).Read
	     6.31s  2.84% 64.98%      6.31s  2.84%  runtime.writebarrierptr
	     6.26s  2.82% 67.80%     32.09s 14.43%  runtime.entersyscall

Often this output is hard to understand.

* Using pprof (cont.)

å¯è§†åŒ–çš„ç»“æœæ›´å®¹æ˜“ç†è§£

	% go tool pprof application /tmp/c.p
	Entering interactive mode (type "help" for commands)
	(pprof) web

Opens a web page with a graphical display of the profile.

.link images/profile.svg 

_Note_: visualisation requires graphviz.

I find this method to be superior to the text mode, I strongly recommend you try it.

pprof also supports these modes in a non interactive form with flags like `-svg`, `-pdf`, etc. See `go`tool`pprof`-help` for more details.

.link http://blog.golang.org/profiling-go-programs Further reading: Profiling Go programs
.link https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs Further reading: Debugging performance issues in Go programs

* Using pprof (cont.) 

The output of a memory profile can be similarly visualised.

    % go build -gcflags='-memprofile=/tmp/m.p'
    % go tool pprof --alloc_objects -svg $(go tool -n compile) /tmp/m.p > alloc_objects.svg
    % go tool pprof --inuse_objects -svg $(go tool -n compile) /tmp/m.p > inuse_objects.svg

Memory profiles come in two varieties

- Alloc objects reports the call site where each allocation was made

.link images/alloc_objects.svg

- Inuse objects reports the call site where an allocation was made _iff_ it was reachable at the end of the profile

.link images/inuse_objects.svg

DEMO: `examples/inuseallocs`

* Using pprof (cont.) 

Here is a visualisation of a block profile:

    % go test -run=XXX -bench=ClientServer -blockprofile=/tmp/b.p net/http
    % go tool pprof -svg http.test /tmp/b.p > block.svg

.link images/block.svg 

* Profiling benchmarks

`testing` åŒ…å†…ç½®äº†ç”ŸæˆCPU,å†…å­˜,é˜»å¡åˆ†æçš„æ”¯æŒ

- `-cpuprofile=$FILE` writes a CPU profile to `$FILE`. 
- `-memprofile=$FILE`, writes a memory profile to `$FILE`, `-memprofilerate=N` adjusts the profile rate to `1/N`.
- `-blockprofile=$FILE`, writes a block profile to `$FILE`.

Using any of these flags also preserves the binary.

    % go test -run=XXX -bench=. -cpuprofile=c.p bytes
    % go tool pprof bytes.test c.p

_Note:_ use `-run=XXX` to disable tests, you only want to profile benchmarks. You can also use `-run=^$` to accomplish the same thing.

* Profiling applications

Profiling `testing` benchmarks is useful for _microbenchmarks_.é™¤äº†testing,æˆ‘ä»¬è¿˜éœ€è¦åˆ†ææ•´ä¸ªç¨‹åºã€‚

We use microbenchmarks inside the standard library to make sure individual packages do not regress, ä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³æµ‹è¯•æ•´ä¸ªç¨‹åºçš„æ€§èƒ½å‘¢?

The Go runtime's profiling interface is in the `runtime/pprof` package.

`runtime/pprof` æ˜¯ä¸ªä½çº§å·¥å…·ã€‚å› ä¸ºå†å²åŸå› ,ä¸åŒç±»å‹çš„åˆ†æä½¿ç”¨çš„æ¥å£å¹¶ä¸ä¸€è‡´ã€‚

A few years ago I wrote a small package, [[https://github.com/pkg/profile][github.com/pkg/profile]], to make it easier to profile an application.

     import "github.com/pkg/profile"

     func main() {
           defer profile.Start().Stop()
           ...
     }

* Exercise

- Generate a profile from a piece of code you know well. If you don't have a code sample, try profiling `godoc`.

- If you were to generate a profile on one machine and inspect it on another, how would you do it?

* Framepointers

Go 1.7å‘å¸ƒçš„åŒäº‹å¸¦äº†ä¸€ä¸ªamd64çš„ç¼–è¯‘å™¨,å®ƒé»˜è®¤å¼€å¯äº†frame pointers.

frame pointeræ˜¯ä¸ªå¯„å­˜å™¨,æ€»æ˜¯æŒ‡å‘å½“å‰æ ˆçš„æœ€é¡¶å¸§(the top of the current stack frame)ã€‚

Framepointers enable tools like `gdb(1)`, and `perf(1)` to understand the Go call stack.

We won't cover these tools in this workshop, but you can read and watch a presentation I gave on seven different ways to profile Go programs.

Further reading:

.link https://talks.godoc.org/github.com/davecheney/presentations/seven.slide Seven ways to profile a Go program (slides)
.link https://www.youtube.com/watch?v=2h_NFBFrciI Video (30 mins)
.link https://www.bigmarker.com/remote-meetup-go/Seven-ways-to-profile-a-Go-program Recording (60 mins)

* go tool trace

In Go 1.5, Dmitry Vyukov added a new kind of profiling to the runtime; [[https://golang.org/doc/go1.5#trace_command][execution trace profiling]].

å¯ä»¥çœ‹åˆ°ä¸€ä¸ªç¨‹åºçš„åŠ¨æ€æ‰§è¡Œã€‚

Captures with nanosecond precision:

- goroutine creation/start/end
- goroutine blocking/unblocking
- network blocking
- system calls
- GC events

Execution traces are essentially undocumented â˜¹ï¸, see [[https://github.com/golang/go/issues/16526][github/go#16526]]

* go tool trace (cont.)

äº§ç”Ÿæ‰§è¡Œè¿½è¸ª(execution trace)

 % cd $(go env GOROOT)/test/bench/go1
 % go test -bench=HTTPClientServer -trace=/tmp/t.p

Viewing the trace:

 % go tool trace /tmp/t.p
 2016/08/13 17:01:04 Parsing trace...
 2016/08/13 17:01:04 Serializing trace...
 2016/08/13 17:01:05 Splitting trace...
 2016/08/13 17:01:06 Opening browser

Bonus: [[https://github.com/pkg/profile/releases/tag/v1.2.0][`github.com/pkg/profile`]] supports generating trace profiles.

 defer profile.Start(profile.TraceProfile).Stop()

* Exercises

- Create a trace profile of your application using `go`tool`trace` and [[https://github.com/pkg/profile/releases/tag/v1.2.0][`github.com/pkg/profile`]].

* Compiler optimisations ç¼–è¯‘å™¨ä¼˜åŒ–

* Compiler optimisations

This section gives a brief background on three important optimisations that the Go compiler performs.

- Escape analysis é€ƒé€¸åˆ†æ
- Inlining å†…è”
- Dead code elimination æ— ç”¨ä»£ç åˆ é™¤

These are all handled in the front end of the compiler, while the code is still in its AST form; then the code is passed to the SSA compiler for further optimisation. 

* Escape analysis

ä¸€ä¸ªåˆé€‚çš„Goçš„å®ç°èƒ½å¤Ÿå‚¨å­˜æ¯æ¬¡å †å†…å­˜åˆ†é…çš„ä¿¡æ¯,ä½†æ˜¯è¿™ä¼šç»™gcé€ æˆè¾ƒå¤§çš„å‹åŠ›ã€‚

ç„¶è€Œæ ˆæ˜¯ä¸ªé€‚åˆå­˜å‚¨å±€éƒ¨å˜é‡çš„å»‰ä»·å­˜å‚¨,å®ƒä¹Ÿä¸éœ€è¦åƒåœ¾æ”¶é›†ã€‚

ä»¥Cå’ŒC++ä¸ºä»£è¡¨çš„è¯­è¨€,å †ã€æ ˆç”³è¯·éƒ½æ˜¯æ‰‹åŠ¨çš„,è¿™æ˜¯é€ æˆå†…å­˜bugçš„ä¸€ä¸ªæ™®éåŸå› ã€‚

åœ¨Goä¸­,å¦‚æœä¸€ä¸ªå˜é‡çš„å­˜æ´»æ—¶é—´è¶…è¿‡äº†å‡½æ•°è°ƒç”¨çš„ç”Ÿå‘½å‘¨æœŸ,ç¼–è¯‘å™¨ä¼šè‡ªåŠ¨æŠŠå®ƒç§»åŠ¨åˆ°å †ä¸­ã€‚è¿™è¢«ç§°ä¸ºè¿™ä¸ªå˜é‡é€ƒé€¸åˆ°äº†å †ä¸­ã€‚

åŒæ—¶,ç¼–è¯‘å™¨ä¹Ÿå¯ä»¥åšç›¸åçš„äº‹æƒ…ã€‚å®ƒå¯ä»¥æŠŠnew,makeç­‰é¢„æœŸç”³è¯·åœ¨å †ä¸Šçš„å†…å­˜ç§»åŠ¨åˆ°æ ˆé‡Œã€‚

* Escape analysis (example)

Sum adds the ints between 1 and 100 and returns the result.

.code examples/esc/sum.go /START OMIT/,/END OMIT/
.caption examples/esc/sum.go

å› ä¸ºnumbersåˆ‡ç‰‡åªå­˜åœ¨äºSumå‡½æ•°ä¸­,ç¼–è¯‘å™¨ä¼šä¸ºè¿™ä¸ªåˆ‡ç‰‡åœ¨æ ˆä¸Šåˆ†é…å­˜å‚¨100ä¸ªæ•´æ•°çš„ç©ºé—´,è€Œä¸æ˜¯å †ã€‚è¿™æ ·å°±ä¸éœ€è¦å¯¹numbersè¿›è¡Œåƒåœ¾æ”¶é›†,Sumå‡½æ•°è¿”å›çš„æ—¶å€™å°±ä¼šè‡ªåŠ¨é‡Šæ”¾æ‰äº†ã€‚

* Investigating escape analysis

Prove it!

To print the compilers escape analysis decisions, use the `-m` flag.

 % go build -gcflags=-m examples/esc/sum.go
 # command-line-arguments
 examples/esc/sum.go:10: Sum make([]int, 100) does not escape
 examples/esc/sum.go:25: Sum() escapes to heap
 examples/esc/sum.go:25: main ... argument does not escape

Line 10 shows the compiler has correctly deduced that the result of `make([]int,`100)` does not escape to the heap.

We'll come back to line 25 soon.

* Escape analysis (example)

This example is a little contrived.

.code  examples/esc/center.go /START OMIT/,/END OMIT/

`NewPoint` creates a new `*Point` value, `p`. We pass `p` to the `Center` function which moves the point to a position in the center of the screen. Finally we print the values of `p.X` and `p.Y`.

* Escape analysis (example)

 % go build -gcflags=-m examples/esc/center.go 
 # command-line-arguments
 examples/esc/center.go:12: can inline Center
 examples/esc/center.go:19: inlining call to Center
 examples/esc/center.go:12: Center p does not escape
 examples/esc/center.go:20: p.X escapes to heap
 examples/esc/center.go:20: p.Y escapes to heap
 examples/esc/center.go:18: NewPoint new(Point) does not escape
 examples/esc/center.go:20: NewPoint ... argument does not escape

Even though `p` was allocated with the `new` function, it will not be stored on the heap, because no reference `p` escapes the `Center` function.

Question: What about line 20, if `p` doesn't escape, what is escaping to the heap?

 examples/esc/center.go:20: p.X escapes to heap
 examples/esc/center.go:20: p.Y escapes to heap

.link https://github.com/golang/go/issues/7714 Escape analysis is not perfect

* Exercise

- What is happening on line 25? Open up `examples/esc/sum.go` and see.
- Write a benchmark to provide that `Sum` does not allocate

* Inlining 

Goçš„å‡½æ•°è°ƒç”¨æœ‰å›ºå®šå¼€é”€,æ ˆå’ŒæŠ¢å æ£€æŸ¥ã€‚ï¼ˆå¯ç†è§£ä¸ºå‡½æ•°è°ƒç”¨å¼€é”€ï¼‰
In Go function calls in have a fixed overhead; stack and preemption check.

æœ‰äº›å¼€é”€å¯ä»¥è¢«ç¡¬ä»¶çš„åˆ†æ”¯é¢„æµ‹æ”¹å–„,ä½†æ˜¯æ ¹æ®å‡½æ•°å¤§å°å’Œæ—¶é’Ÿå‘¨æœŸæ¥ç®—,è¿˜æ˜¯æœ‰å¼€é”€ã€‚ but it's still a cost in terms of function size and clock cycles.

Inliningæ˜¯ç»å…¸çš„é¿å…è¿™äº›å¼€é”€çš„æ‰‹æ®µã€‚

Inliningåªå½±å“å¶å­å‡½æ•°,å°±æ˜¯é‚£äº›ä¸è°ƒç”¨å…¶ä»–å‡½æ•°çš„å‡½æ•°ã€‚åˆ¤æ–­æ ‡å‡†å¦‚ä¸‹:

- å¦‚æœå‡½æ•°è¦åšçš„åŠŸèƒ½æ¯”è¾ƒå¤š,é‚£è°ƒç”¨å¼€é”€å°±å¯ä»¥å¿½ç•¥ã€‚æ‰€ä»¥å‡½æ•°è¶…è¿‡æŸä¸ªå¤§å°å°±ä¸ä¼šè¿›è¡Œå†…è”ã€‚That's why functions over a certain size (currently some count of instructions, plus a few operations which prevent inlining all together (eg. switch before Go 1.7)
- ç›¸å¯¹æ¥è¯´,ç›¸æ¯”äºçœŸæ­£çš„åŠŸèƒ½,å°å‡½æ•°ä¹Ÿæœ‰åŒæ ·å›ºå®šçš„è°ƒç”¨å¼€é”€ã€‚è¿™äº›å‡½æ•°å»åšå†…è”ä¼šæœ‰æ›´å¤šå¥½å¤„ã€‚

è¿˜æœ‰ä¸ªåŸå› æ˜¯å®ƒè®©å †æ ˆä¿¡æ¯éš¾ä»¥è¿½è¸ªã€‚

* Inlining (example)

.play examples/max/max.go /START OMIT/,/END OMIT/

* Inlining (cont.)

Again we use the `-m` flag to view the compilers optimisation decision.

 % go build -gcflags=-m examples/max/max.go 
 # command-line-arguments
 examples/max/max.go:4: can inline Max
 examples/max/max.go:13: inlining call to Max

Compile `max.go` and see what the optimised version of `F()` became.

DEMO: `go`build`-gcflags="-m`-S"`examples/max/max.go`2>&1`|`less`

* Discussion

- Why did I declare `a` and `b` in `F()` to be constants?
- What happens if they are variables?
- What happens if they are passing into `F()` as parameters?

* Dead code elimination

Why is it important that `a` and `b` are constants?

After inlining, this is what the compiler saw

.play examples/max/max2.go /START OMIT/,/END OMIT/

- The call to `Max` has been inlined.
- If `a`>`b` then there is nothing to do, so the function returns. 
- If `a`<`b` then the branch is false and we fall through to `panic`
- But, because `a` and `b` are constants, we know that the branch will never be false, so the compiler can optimise `F()` to a return.

* Dead code elimination (cont.)

æ— ç”¨ä»£ç åˆ é™¤å’Œå†…è”ä¸€èµ·é€šè¿‡åˆ é™¤é‚£äº›ä¸å¯è¾¾çš„å¾ªç¯å’Œåˆ†æ”¯æ¥å‡å°‘ä»£ç æ•°é‡ã€‚

You can take advantage of this to implement expensive debugging, and hide it behind

 const debug = false 

Combined with build tags this can be very useful.

Further reading:

.link http://dave.cheney.net/2014/09/28/using-build-to-switch-between-debug-and-release Using // +build to switch between debug and release builds
.link http://dave.cheney.net/2013/10/12/how-to-use-conditional-compilation-with-the-go-build-tool How to use conditional compilation with the go build tool

* Compiler flags Exercises

Compiler flags are provided with:

 go build -gcflags=$FLAGS

Investigate the operation of the following compiler functions:

- `-S` prints the (Go flavoured) assembly of the _package_ being compiled.
- `-l` controls the behaviour of the inliner; `-l` disables inlining, `-l`-l` increases it (more `-l` 's increases the compiler's appetite for inlining code). Experiment with the difference in compile time, program size, and run time.
- `-m` controls printing of optimisation decision like inlining, escape analysis. `-m`-m` prints more details about what the compiler was thinking.
- `-l`-N` disables all optimisations.

.link http://go-talks.appspot.com/github.com/rakyll/talks/gcinspect/talk.slide#1 Further reading: Codegen Inspection by Jaana Burcu Dogan

Perhaps we shall take a break now.

* Memory management and GC tuning å†…å­˜ç®¡ç†å’ŒGCè°ƒä¼˜

* Memory management and GC tuning

Goæ˜¯ä¸€ä¸ªåƒåœ¾æ”¶é›†çš„è¯­è¨€,è¿™æ˜¯ä¸ä¼šæ”¹å˜çš„è®¾è®¡åŸåˆ™ã€‚

ä½œä¸ºä¸€ä¸ªå¸¦åƒåœ¾æ”¶é›†çš„è¯­è¨€,Goç¨‹åºçš„æ€§èƒ½ç»å¸¸ä¼šè¢«åƒåœ¾æ”¶é›†å™¨çš„è¡Œä¸ºæ‰€å½±å“ã€‚

é™¤äº†ç®—æ³•é€‰æ‹©,å†…å­˜æ¶ˆè€—æ˜¯å†³å®šæ€§èƒ½å’Œå¯æ‰©å±•æ€§(performance and scalability)æœ€é‡è¦çš„å› ç´ 

æœ¬èŠ‚è®¨è®ºåƒåœ¾æ”¶é›†å™¨çš„è¡Œä¸º,å¦‚ä½•æµ‹é‡ç¨‹åºçš„å†…å­˜ä½¿ç”¨é‡,ä»¥åŠåœ¨åƒåœ¾æ”¶é›†å™¨æ€§èƒ½æˆä¸ºç“¶é¢ˆçš„æ—¶å€™å†…å­˜ä½¿ç”¨ç­–ç•¥(strategies for lowering memory usage)

* Garbage collector world view

åƒåœ¾æ”¶é›†å™¨çš„ç›®æ ‡æ˜¯ç»™ç¨‹åºé€ æˆä¸€ç§æœ‰æ— é™å†…å­˜å¯ä»¥ä½¿ç”¨çš„å‡è±¡ã€‚

You may disagree with this statement, but this is the base assumption of how garbage collector designers think.

# A stop the world, mark sweep GC is the most efficient in terms of total run time; good for batch processing, simulation, etc.

Go GCçš„è®¾è®¡ç”¨é€”æ˜¯ä½å»¶è¿ŸæœåŠ¡å™¨å’Œäº¤äº’å¼ç¨‹åºã€‚

ç›¸å¯¹äºé«˜ååé‡,Go GCæ›´åå‘äºä½å»¶è¿Ÿã€‚å®ƒæŠŠä¸€äº›å†…å­˜åˆ†é…å¼€é”€å˜æˆä¿®æ”¹(mutator),ä»¥æ­¤æ§åˆ¶ä¹‹åçš„æ¸…é™¤å¼€é”€ã€‚

* Garbage collector design

The design of the Go GC has changed over the years

- Go 1.0, stop the world mark sweep collector based heavily on tcmalloc.
- Go 1.3, fully precise collector, wouldn't mistake big numbers on the heap for pointers, thus leaking memory.
- Go 1.5, new GC design, focusing on _latency_ over _throughput_.
- Go 1.6, GC improvements, handling larger heaps with lower latency.
- Go 1.7, small GC improvements, mainly refactoring.
- Go 1.8, further work to reduce STW times, now down to the 100 microsecond range.
- Go 1.9, ROC collector is an experiment to extend the idea of escape analysis per goroutine.

* Garbage collector monitoring

A simple way to obtain a general idea of how hard the garbage collector is working is to enable the output of GC logging.

These stats are always collected, but normally suppressed, you can enable their display by setting the `GODEBUG` environment variable.

 % env GODEBUG=gctrace=1 godoc -http=:8080
 gc 1 @0.017s 8%: 0.021+3.2+0.10+0.15+0.86 ms clock, 0.043+3.2+0+2.2/0.002/0.009+1.7 ms cpu, 5->6->1 MB, 4 MB goal, 4 P
 gc 2 @0.026s 12%: 0.11+4.9+0.12+1.6+0.54 ms clock, 0.23+4.9+0+3.0/0.50/0+1.0 ms cpu, 4->6->3 MB, 6 MB goal, 4 P
 gc 3 @0.035s 14%: 0.031+3.3+0.76+0.17+0.28 ms clock, 0.093+3.3+0+2.7/0.012/0+0.84 ms cpu, 4->5->3 MB, 3 MB goal, 4 P
 gc 4 @0.042s 17%: 0.067+5.1+0.15+0.29+0.95 ms clock, 0.20+5.1+0+3.0/0/0.070+2.8 ms cpu, 4->5->4 MB, 4 MB goal, 4 P
 gc 5 @0.051s 21%: 0.029+5.6+0.33+0.62+1.5 ms clock, 0.11+5.6+0+3.3/0.006/0.002+6.0 ms cpu, 5->6->4 MB, 5 MB goal, 4 P
 gc 6 @0.061s 23%: 0.080+7.6+0.17+0.22+0.45 ms clock, 0.32+7.6+0+5.4/0.001/0.11+1.8 ms cpu, 6->6->5 MB, 7 MB goal, 4 P
 gc 7 @0.071s 25%: 0.59+5.9+0.017+0.15+0.96 ms clock, 2.3+5.9+0+3.8/0.004/0.042+3.8 ms cpu, 6->8->6 MB, 8 MB goal, 4 P

The trace output gives a general measure of GC activity.

DEMO: Show `godoc` with `GODEBUG=gctrace=1` enabled

_Recommendation_: use this env var in production, it has no performance impact.

* Garbage collector monitoring (cont.)

Using `GODEBUG=gctrace=1` is good when you _know_ there is a problem, but for general telemetry on your Go application I recommend the `net/http/pprof` interface.

    import _ "net/http/pprof"

Importing the `net/http/pprof` package will register a handler at `/debug/pprof` with various runtime metrics, including:

- A list of all the running goroutines, `/debug/pprof/heap?debug=1`. 
- A report on the memory allocation statistics, `/debug/pprof/heap?debug=1`.

*Warning*: `net/http/pprof` will register itself with your default `http.ServeMux`.

Be careful as this will be visible if you use `http.ListenAndServe(address,`nil)`.

DEMO: `godoc`-http=:8080`, show `/debug/pprof`.

* Garbage collector tuning

Goè¿è¡Œæ—¶ä¸ºGCè°ƒä¼˜æä¾›äº†ä¸€ä¸ªç¯å¢ƒå˜é‡,GOGC

The formula for GOGC is as follows.

    goal = reachable * (1 + GOGC/100)

For example, if we currently have a 256MB heap, and `GOGC=100` (the default), when the heap fills up it will grow to

    512MB = 256MB * (1 + 100/100)

- `GOGC` çš„å€¼è¶…è¿‡100çš„æ—¶å€™,å †å¢é•¿å¿«,GCå‹åŠ›å˜ä½ã€‚
- `GOGC` çš„å€¼ä½äº100çš„æ—¶å€™,å †å¢é•¿æ…¢,GCå‹åŠ›å˜é«˜ã€‚

The default value of 100 is _just_a_guide_. you should choose your own value _after_profiling_your_application_with_production_loads_.

* Reduce allocations

ç¡®å®šä½ çš„APIæ¥å£èƒ½è®©è°ƒç”¨è€…å‡å°‘ç”Ÿæˆçš„åƒåœ¾ã€‚

Consider these two Read methods

    func (r *Reader) Read() ([]byte, error)
    func (r *Reader) Read(buf []byte) (int, error)

The first Read method takes no arguments and returns some data as a `[]byte`. The second takes a `[]byte` buffer and returns the amount of bytes read.

The first Read method will _always_ allocate a buffer, putting pressure on the GC. The second fills the buffer it was given.

_Exercise_: Can you name examples in the std lib which follow this pattern?

* strings and []bytes

Goä¸­stringæ˜¯ä¸å¯å˜çš„,[]byteæ˜¯å¯å˜çš„ã€‚

å¤§éƒ¨åˆ†ç¨‹åºæ›´å–œæ¬¢ç”¨string,è€ŒIOéƒ½æ˜¯ç”¨[]byteå†™çš„ã€‚

å°½é‡é¿å…[]byteåˆ°stringçš„è½¬æ¢ã€‚è¿™é€šå¸¸æ„å‘³ç€ä¸ºä¸€ä¸ªå€¼é€‰ä¸€ç§è¡¨è¾¾æ–¹å¼,stringæˆ–è€…[]byteã€‚å¦‚æœæ•°æ®æ˜¯ä»ç½‘ç»œæˆ–è€…ç£ç›˜è¯»åˆ°çš„,é€šå¸¸æ˜¯[]byteã€‚

[[https://golang.org/pkg/bytes/][`bytes`]] åŒ…é‡Œé¢æœ‰å¾ˆå¤šå’Œ[[https://golang.org/pkg/strings/][`strings`]]åŒ…ç›¸åŒçš„æ“ä½œ â€” `Split`, `Compare`, `HasPrefix`, `Trim`, etc

Under the hood `strings` uses same assembly primitives as the `bytes` package.

* Using []byte as a map key

It is very common to use a `string` as a map key, but often you have a `[]byte`.

The compiler implements a specific optimisation for this case

     var m map[string]string
     v, ok := m[string(bytes)]

This will avoid the conversion of the byte slice to a string for the map lookup. This is very specific, it won't work if you do something like

     key := string(bytes)
     val, ok := m[key] 

* Avoid string concatenation

Go stringsæ˜¯ä¸å¯å˜çš„ã€‚è¿æ¥ä¸¤ä¸ªå­—ç¬¦ä¸²ä¼šäº§ç”Ÿç¬¬ä¸‰ä¸ª,å“ªä¸ªä¼šæœ€å¿«?

.code examples/concat/concat_test.go /START1 OMIT/,/END1 OMIT/
.code examples/concat/concat_test.go /START2 OMIT/,/END2 OMIT/
.code examples/concat/concat_test.go /START3 OMIT/,/END3 OMIT/
.code examples/concat/concat_test.go /START4 OMIT/,/END4 OMIT/

DEMO: `go`test`-bench=.`./examples/concat`

* Preallocate slices if the length is known

Appendå¾ˆæ–¹ä¾¿,ä½†æ˜¯å¾ˆæµªè´¹ã€‚

åˆ‡ç‰‡æ¯æ¬¡ç¿»å€å¢åŠ ç›´åˆ°1024ä¸ªå…ƒç´ ,ä¹‹åå¤§æ¦‚æ¯æ¬¡å¢åŠ 25%ã€‚bçš„å®¹é‡åœ¨appendä¸€ä¸ªå…ƒç´ åä¼šæ˜¯å¤šå°‘?

.play examples/grow.go /START OMIT/,/END OMIT/

If you use the append pattern you could be copying a lot of data and creating a lot of garbage.

* Preallocate slices if the length is known (cont.)

If know know the length of the slice beforehand, then pre-allocate the target to avoid copying and to make sure the target is exactly the right size. 

_Before:_

     var s []string
     for _, v := range fn() {
            s = append(s, v)
     }
     return s

_After:_

     vals := fn()
     s := make([]string, len(vals))
     for i, v := range vals {
            s[i] = v           
     }
     return s

* Using sync.Pool

The `sync` package comes with a `sync.Pool` type which is used to reuse common objects.

`sync.Pool` has no fixed size or maximum capacity. You add to it and take from it until a GC happens, then it is emptied unconditionally. 

.code examples/pool.go /START OMIT/,/END OMIT/

*Warning*: `sync.Pool` is not a cache. It can and will be emptied _at_any_time_.

Do not place important items in a `sync.Pool`, they will be discarded.

_Personal_opinion_: `sync.Pool` å¾ˆéš¾å®‰å…¨ä½¿ç”¨,ä¸è¦ç”¨å®ƒã€‚

* Exercises

- Using `godoc` (or another program) observe the results of changing `GOGC` using `GODEBUG=gctrace=1`.

- Benchmark byte's string(byte) map keys

- Benchmark allocs from different concat strategies.

* Concurrency å¹¶å‘

* Concurrency

Goçš„æ ‡å¿—æ€§ç‰¹æ€§å°±æ˜¯å®ƒçš„è½»é‡çº§å¹¶å‘æ¨¡å‹ã€‚

è™½ç„¶å¾ˆå»‰ä»·,ä½†ä¹Ÿä¸æ˜¯æ²¡æœ‰ä»£ä»·ã€‚è¿‡åº¦ä½¿ç”¨ä¾ç„¶ä¼šå¯¼è‡´ä¸å¿…è¦çš„æ€§èƒ½é—®é¢˜ã€‚

è¿™èŠ‚ä¼šæœ‰ä¸€ç³»åˆ—ä¸ºäº†æ›´å¥½ä½¿ç”¨åŸç”ŸGoå¹¶å‘çš„å»ºè®®å’Œç¦æ­¢ã€‚

* Goroutines

goroutinesæ˜¯goè¯­è¨€é€‚åº”ç°ä»£ç¡¬ä»¶çš„å…³é”®ç‰¹æ€§ã€‚

Goroutineséå¸¸æ˜“ç”¨,åˆ›å»ºéå¸¸å»‰ä»·,ä½ å¯ä»¥è®¤ä¸ºå®ƒå‡ ä¹æ²¡æœ‰ä»£ä»·ã€‚

Goçš„è¿è¡Œæ—¶ç¯å¢ƒèƒ½æ‰¿æ‹…å¸¸æ€åŒ–æ•°ä¸‡çš„goroutines,ä½†æ˜¯ä¸å»ºè®®åˆ›å»ºåˆ°æ•°åä¸‡ã€‚

ç„¶è€Œ,æ¯ä¸ªgoroutineæœ€å°‘éœ€è¦ä½¿ç”¨2kå†…å­˜ä½œä¸ºå®ƒçš„æ ˆå†…å­˜ã€‚

2048 * 1,000,000 goroutines == 2GB,ç°åœ¨è¿˜ä»€ä¹ˆéƒ½æ²¡å¹²ã€‚

# Maybe this is a lot, maybe it isn't given the other usages of your application

* Know when to stop a goroutine

Goroutinesçš„åˆ›å»ºå’Œè¿è¡Œéƒ½å¾ˆå»‰ä»·,ä½†æ˜¯å› ä¸ºå†…å­˜é—®é¢˜ä¾ç„¶æœ‰ä¸ªæœ‰é™çš„ä»£ä»·ã€‚å®ƒä¸èƒ½è¢«æ— é™çš„åˆ›å»ºã€‚

æ¯ä¸ªä½ ä½¿ç”¨goå…³é”®è¯æ¥åˆ›å»ºçš„goroutine,ä½ å¿…é¡»çŸ¥é“æ€æ ·,ä½•æ—¶èƒ½å¤Ÿè®©å®ƒé€€å‡ºã€‚

å¦‚æœä½ ä¸çŸ¥é“,é‚£è¿™å°±ä¼šæ˜¯ä¸ªæ½œåœ¨çš„å†…å­˜æ³„éœ²ã€‚

åœ¨ä½ çš„è®¾è®¡ä¸­,æŸäº›goroutineså¯èƒ½ä¼šä¸€ç›´è¿è¡Œåˆ°ç¨‹åºé€€å‡ºã€‚è¿™äº›goroutinesè¦è¶³å¤Ÿå°‘,ä»¥é¿å…æˆä¸ºè¿™ä¸ªè§„åˆ™ä¸‹çš„å¼‚å¸¸ã€‚

*Never*start*a*goroutine*without*knowing*how*it*will*stop*.

# * Know when to stop a goroutine (cont.)
#
#TODO SHOW HOW TO STOP A GOROUTINE USING A DONE CHANNEL

* Go uses efficient network polling for some requests

Goè¿è¡Œæ—¶åœ¨å¤„ç†ç½‘ç»œIOçš„æ—¶å€™ä½¿ç”¨äº†éå¸¸é«˜æ•ˆçš„ç³»ç»Ÿpollingæœºåˆ¶(kqueue, epoll, windows IOCPç­‰)ã€‚å¾ˆå¤šç­‰å¾…ä¸­çš„goroutineså¯ä»¥å¯¹åº”åŒä¸€ä¸ªç³»ç»Ÿçº¿ç¨‹ã€‚

ç„¶è€Œå¯¹äºæœ¬åœ°æ–‡ä»¶IO,Goå¹¶æ²¡æœ‰å®ç°IO pollingã€‚æ¯ä¸ªå¯¹äº*os.Fileçš„æ“ä½œéƒ½ä¼šåœ¨æ‰§è¡Œä¸­ä½¿ç”¨ç³»ç»Ÿçº¿ç¨‹ã€‚

å¤§é‡ä½¿ç”¨æœ¬åœ°æ–‡ä»¶IOä¼šå¯¼è‡´ç¨‹åºäº§ç”Ÿå‡ºæ•°åä¸‡çš„çº¿ç¨‹,å¯èƒ½æ¯”ç³»ç»Ÿå…è®¸çš„è¿˜è¦å¤šã€‚ possibly more than your operating system allows.

ä½ çš„ç£ç›˜å­ç³»ç»Ÿå¹¶ä¸æ˜¯ç»™å¹¶å‘å¤„ç†æ•°åä¸‡IOè¯·æ±‚è€Œè®¾è®¡çš„ã€‚

* io.Reader and io.Writer are not buffered

`io.Reader` and `io.Writer` implementations are not buffered.
	
This includes `net.Conn` and `os.Stdout`.

Use `bufio.NewReader(r)` and `bufio.NewWriter(w)` to get a buffered reader and writer.

Don't forget to `Flush` or `Close` your buffered writers to flush the buffer to the underlying `Writer`.

* Watch out for IO multipliers in your application

å¦‚æœä½ åœ¨å†™æœåŠ¡ç«¯è¿›ç¨‹,ä¸»è¦å·¥ä½œæ˜¯å¤šè·¯ä¼ è¾“(multiplex)é€šè¿‡ç½‘ç»œè¿æ¥çš„å®¢æˆ·ç«¯,å’Œç¨‹åºä¸­å‚¨å­˜çš„æ•°æ®ã€‚

å¤§å¤šæ•°çš„æœåŠ¡å™¨ç¨‹åºæ¥æ”¶ä¸€ä¸ªè¯·æ±‚,åšäº›å¤„ç†,ç„¶åè¿”å›ä¸€ä¸ªç»“æœã€‚å¬èµ·æ¥å¾ˆç®€å•,ä½†æ˜¯æ ¹æ®ç»“æœä¸åŒå¯ä»¥è®©å®¢æˆ·ç«¯æ¶ˆè€—æ‰å¤§é‡çš„(ç”šè‡³æ˜¯æ— é™çš„)æœåŠ¡å™¨èµ„æºã€‚ä¸‹é¢æ˜¯ä¸€äº›éœ€è¦æ³¨æ„çš„ç‚¹:

- æ¯ä¸ªæ¥çš„è¯·æ±‚(incoming request)éœ€è¦çš„IOè¯·æ±‚(IO requests)çš„æ•°é‡ã€‚å•ä¸ªå®¢æˆ·ç«¯è¯·æ±‚éœ€è¦å¤šå°‘IOäº‹ä»¶?å¯èƒ½å¹³å‡1ä¸ª,æˆ–è€…åœ¨å¤šæ•°è¯·æ±‚è¢«cacheæŒ¡ä½æ—¶ä¼šå°‘äº1.
- æ¯ä¸ªæŸ¥è¯¢(query)éœ€è¦å¤šå°‘è¯»(amount of reads)ã€‚æ˜¯å¸¸é‡,N+1,è¿˜æ˜¯çº¿æ€§(è¯»å–æ•´ä¸ªè¡¨æ¥ç”Ÿæˆæœ€åä¸€é¡µç»“æœ)?

ç›¸å¯¹æ¥è¯´,å¦‚æœå†…å­˜æ…¢,é‚£IOå°±æ˜¯éå¸¸æ…¢,ä»¥è‡³äºä½ åº”è¯¥ä»˜å‡ºä»»ä½•ä»£ä»·æ¥é¿å…å®ƒã€‚æœ€é‡è¦çš„æ˜¯ä¸è¦åœ¨è¯·æ±‚çš„ä¸Šä¸‹æ–‡ä¸­è¿›è¡ŒIOè¯·æ±‚,ä¸è¦è®©ç”¨æˆ·ç­‰å¾…ä½ çš„ç£ç›˜å­ç³»ç»Ÿå†™å…¥ç£ç›˜,ç”šè‡³è¯»ã€‚

* Use streaming IO interfaces

åªè¦æœ‰å¯èƒ½,é¿å…æŠŠæ•°æ®è¯»å…¥[]byteå¹¶ä¸”ä¼ æ¥ä¼ å»ã€‚

ä¸€ä¸ªè¯·æ±‚ä¸­,ä½ å¯èƒ½æœ€åä¼šè¯»å–æ•°å…†çš„æ•°æ®åˆ°å†…å­˜ä¸­,è¿™ä¼šç»™GCå¸¦æ¥å·¨å¤§çš„å‹åŠ›,ä¹Ÿä¼šå¢åŠ ç¨‹åºçš„å¹³å‡å»¶æ—¶.

Instead use `io.Reader` and `io.Writer` to construct processing pipelines to cap the amount of memory in use per request.

For efficiency, consider implementing `io.ReaderFrom` / `io.WriterTo` if you use a lot of `io.Copy`. è¿™äº›æ¥å£æ•ˆç‡æ›´å¥½,å¹¶ä¸”é¿å…æŠŠå†…å­˜æ‹·è´åˆ°ä¸´æ—¶bufferé‡Œã€‚

* Timeouts, timeouts, timeouts

æ°¸è¿œä¸è¦åœ¨ä¸çŸ¥é“ä¸€ä¸ªIOæ“ä½œèŠ±è´¹å¤šä¹…çš„æ—¶å€™æ¥ä½¿ç”¨å®ƒã€‚

You need to set a timeout on every network request you make with `SetDeadline`, `SetReadDeadline`, `SetWriteDeadline`.

é˜»å¡IOçš„æ•°é‡éœ€è¦è¢«é™åˆ¶ã€‚ç”¨ä¸€ä¸ªgoroutineæ± ,æˆ–è€…ç”¨æœ‰ç¼“å†²çš„channelä½œä¸ºä¿¡å·é‡ã€‚

.code examples/semaphore.go /START OMIT/,/END OMIT/

* Defer is expensive, or is it?

`defer` is expensive because it has to record a closure for defer's arguments.

 defer mu.Unlock()

is equivalent to
 
 defer func() {
         mu.Unlock()
 }()

# talk about unwinding costs

å¦‚æœè¦åšçš„äº‹å¾ˆå°,deferçš„ä»£ä»·å°±å¾ˆæ˜‚è´µã€‚deferçš„ç»å…¸ä¾‹å­æ˜¯æ“ä½œstructå˜é‡æˆ–è€…mapæŸ¥æ‰¾æ—¶çš„è§£é”ã€‚åœ¨è¿™äº›æƒ…å†µä¸­ä½ å¯ä»¥é€‰æ‹©ä¸ç”¨deferã€‚

This is a case where readability and maintenance is sacrificed for a performance win. 

Always revisit these decisions.

.link https://github.com/golang/go/issues/9704#issuecomment-251003577

* Minimise cgo

cgo allows Go programs to call into C libraries. 

C code and Go code live in two different universes, cgo traverses the boundary between them.

This transition is not free and depending on where it exists in your code, the cost could be substantial.

cgo calls are similar to blocking IO, they consume a thread during operation.

Do not call out to C code in the middle of a tight loop.

* Actually, avoid cgo

cgo has a high overhead.

For best performance I recommend avoiding cgo in your applications.

- If the C code takes a long time, cgo overhead is not as important.
- If you're using cgo to call a very short C function, where the overhead is the most noticeable, rewrite that code in Go -- by definition it's short.
- If you're using a large piece of expensive C code is called in a tight loop, why are you using Go?

Is there anyone who's using cgo to call expensive C code frequently?

.link http://dave.cheney.net/2016/01/18/cgo-is-not-go Further reading: cgo is not Go.

* Always use the latest released version of Go

Old versions of Go will never get better. They will never get bug fixes or optimisations.

- Go 1.4 should not be used.
- Go 1.5 and 1.6 had a slower compiler, but it produces faster code, and has a faster GC.
- Go 1.7 delivered roughly a 30% improvement in compilation speed over 1.6, a 2x improvement in linking speed (better than any previous version of Go).
- Go 1.8 will deliver a smaller improvement in compilation speed (at this point), but a significant improvement in code quality for non Intel architectures.

Old version of Go receive no updates. Do not use them. Use the latest and you will get the best performance.

.link http://dave.cheney.net/2016/04/02/go-1-7-toolchain-improvements Go 1.7 toolchain improvements
.link http://dave.cheney.net/2016/09/18/go-1-8-performance-improvements-one-month-in Go 1.8 performance improvements

* Discussion

Any questions?

* Conclusion æ€»ç»“

* Conclusion

Always write the simplest code you can, the compiler is optimised for _normal_ code.

Start with the simplest possible code.

_Measure_.

å¦‚æœæ€§èƒ½è¶³å¤Ÿå¥½,é‚£å°±ä¸è¦å†ä¼˜åŒ–äº†ã€‚å¹¶ä¸æ˜¯æ‰€æœ‰ä¸œè¥¿éƒ½éœ€è¦ä¼˜åŒ–,åªæœ‰ä»£ç ä¸­æœ€çƒ­çš„éƒ¨åˆ†ã€‚

éšç€ç¨‹åºçš„å¢é•¿,æˆ–è€…æµé‡æ¨¡å¼çš„è¿›åŒ–,æ€§èƒ½çƒ­ç‚¹ä¼šå˜åŒ–ã€‚

å¦‚æœä¸æ˜¯å½±å“å…³é”®æ€§èƒ½,ä¸è¦ç•™ä¸‹å¤æ‚ä»£ç ã€‚å¦‚æœç“¶é¢ˆä¸å†è¿™é‡Œ,å°±ç”¨ç®€å•æ“ä½œå»é‡å†™ã€‚

* Conclusion (cont.)

ç»™ä»£ç åšæ€§èƒ½åˆ†ææ¥ç¡®å®šç“¶é¢ˆ,ä¸è¦é çŒœã€‚

çŸ­ä»£ç å°±æ˜¯å¿«ä»£ç ã€‚Goä¸æ˜¯C++, do not expect the compiler to unravel complicated abstractions.

Shorter code is _smaller_ code; which is important for the CPU's cache.

Pay very close attention to allocations, avoid unnecessary allocation where possible.

* Don't trade performance for reliability

"I can make things very fast if they don't have to be correct."
.caption Russ Cox

"Readable means reliable"
.caption Rob Pike

æ€§èƒ½å’Œå¯é æ€§åŒç­‰é‡è¦ã€‚

I see little value in making a very fast server that panics, deadlocks or OOMs on a regular basis.

ä¸è¦ç‰ºç‰²å¯é æ€§æ¥æ¢å–æ€§èƒ½ã€‚
